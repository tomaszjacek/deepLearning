{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401b9df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import optimizers, metrics, losses\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,Conv1DTranspose,AveragePooling1D,GlobalMaxPool1D,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Conv2DTranspose,AveragePooling2D,GlobalMaxPool2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv3D,MaxPooling3D,Conv3DTranspose,AveragePooling3D,GlobalMaxPool3D,GlobalAveragePooling3D\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout,Concatenate,Layer,BatchNormalization,Input,Add,Activation,Average\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764fe6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\\tmp\\datasets\\tinyDexinedDataset2test_edge\n"
     ]
    }
   ],
   "source": [
    "#data_path = \"outlineOutput\"\n",
    "#data_path = \"H:\\\\download\\\\blender\\\\projects\\\\vdmTests\\\\outlineOutput\"\n",
    "#output_path = \"H:\\\\tmp\\\\dexined\"\n",
    "\n",
    "data_path = \"I:\\\\tmp\\\\datasets\\\\tinyDexinedDataset2\"\n",
    "output_path = \"I:\\\\tmp\\\\teed\"\n",
    "\n",
    "train_path = data_path + \"train\"\n",
    "edge_train_path = data_path + \"train_edge\"\n",
    "\n",
    "test_path = data_path + \"test\"\n",
    "edge_test_path = data_path + \"test_edge\"\n",
    "\n",
    "val_path = data_path + \"val\"\n",
    "edge_val_path = data_path + \"val_edge\"\n",
    "\n",
    "print(edge_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975803a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "img_shape =None\n",
    "\n",
    "class DataLoader(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self,data_dir, list_name, data_name, image_width, image_height, batch_size, shuffle,crop_img=False):\n",
    "        self.base_dir = data_dir\n",
    "        self.list_name = list_name\n",
    "        self.dim_w = image_width\n",
    "        self.dim_h = image_height\n",
    "        self.data_name =data_name\n",
    "        self.bs = batch_size\n",
    "        self.shuffle=shuffle\n",
    "        self.crop_img = crop_img\n",
    "        self.data_list = self._build_index()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "\n",
    "    def _build_index(self):\n",
    "\n",
    "        file_path = os.path.join(self.base_dir, self.list_name)\n",
    "        with open(file_path,'r') as f:\n",
    "            file_list = f.readlines()\n",
    "        file_list = [line.strip() for line in file_list] # to clean the '\\n'\n",
    "        file_list = [line.split(' ') for line in file_list] # separate paths\n",
    "\n",
    "        input_path = [os.path.join(\n",
    "            self.base_dir,line[0]) for line in file_list]\n",
    "        gt_path = [os.path.join(\n",
    "            self.base_dir,line[1]) for line in file_list]\n",
    "        \n",
    "        sample_indeces= [input_path, gt_path]\n",
    "        return sample_indeces\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.data_list[0]))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)//self.bs\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(\"__getitem__\",index)\n",
    "        indices = self.indices[index*self.bs:(index+1)*self.bs]\n",
    "\n",
    "        x_list,y_list = self.data_list\n",
    "        tmp_x_path = [x_list[k] for k in indices]\n",
    "        tmp_y_path = [y_list[k] for k in indices]\n",
    "\n",
    "        x,y = self.__data_generation(tmp_x_path,tmp_y_path)\n",
    "\n",
    "        return x,y\n",
    "\n",
    "    def __data_generation(self,x_path,y_path):\n",
    "\n",
    "        x = np.empty((self.bs, self.dim_h, self.dim_w, 3), dtype=\"float32\")\n",
    "        y = np.empty((self.bs, self.dim_h, self.dim_w, 1), dtype=\"float32\")\n",
    "\n",
    "        for i,tmp_data in enumerate(x_path):\n",
    "            tmp_x_path = tmp_data\n",
    "            tmp_y_path = y_path[i]\n",
    "            tmp_x,tmp_y = self.transformer(tmp_x_path,tmp_y_path)\n",
    "            x[i,]=tmp_x\n",
    "            y[i,]=tmp_y\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        return x,y\n",
    "\n",
    "    def transformer(self, x_path, y_path):\n",
    "        tmp_x = cv2.imread(x_path)\n",
    "        tmp_y = cv2.imread(y_path,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        h,w,_ = tmp_x.shape\n",
    "\n",
    "        if self.crop_img:\n",
    "            i_h = random.randint(0,h-self.dim_h)\n",
    "            i_w = random.randint(0,w-self.dim_w)\n",
    "            tmp_x = tmp_x[i_h:i_h+self.dim_h,i_w:i_w+self.dim_w,]\n",
    "            tmp_y = tmp_y[i_h:i_h+self.dim_h,i_w:i_w+self.dim_w,]\n",
    "        else:\n",
    "            tmp_x = cv2.resize(tmp_x,(self.dim_w,self.dim_h))\n",
    "            tmp_y = cv2.resize(tmp_y,(self.dim_w,self.dim_h))\n",
    "\n",
    "        tmp_y = np.expand_dims(np.float32(tmp_y)/255.,axis=-1)\n",
    "        tmp_x = np.float32(tmp_x)\n",
    "        return tmp_x, tmp_y\n",
    "\n",
    "\n",
    "def preprocess(x,y,H=512, W=912):\n",
    "\n",
    "    def f(x,y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "\n",
    "        x = read_image(x)\n",
    "        y = read_edge(y)\n",
    "        return x, y\n",
    "\n",
    "    images, edges = tf.numpy_function(f, [x, y], [tf.float32, tf.float32])\n",
    "    images.set_shape([H, W, 3])\n",
    "    edges.set_shape([H, W, 1])\n",
    "    return images, edges\n",
    "\n",
    "def tf_data(x,y,bs):\n",
    "    data = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    data = data.shuffle(buffer_size=bs)\n",
    "    data = data.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    data = data.batch(12)\n",
    "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1226371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I:\\tmp\\datasets\\tinyDexinedDataset2/test_edge/*\n",
      "4 2\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "#data_path = \"outlineOutput\"\n",
    "#data_path = \"H:\\\\download\\\\blender\\\\projects\\\\vdmTests\\\\outlineOutput\"\n",
    "#output_path = \"H:\\\\tmp\\\\dexined\"\n",
    "\n",
    "data_path = \"I:\\\\tmp\\\\datasets\\\\tinyDexinedDataset2\"\n",
    "output_path = \"I:\\\\tmp\\\\teed\"\n",
    "\n",
    "train_path = data_path + \"/train/*\"\n",
    "edge_train_path = data_path + \"/train_edge/*\"\n",
    "\n",
    "test_path = data_path + \"/test/*\"\n",
    "edge_test_path = data_path + \"/test_edge/*\"\n",
    "\n",
    "val_path = data_path + \"/val/*\"\n",
    "edge_val_path = data_path + \"/val_edge/*\"\n",
    "\n",
    "print(edge_test_path)\n",
    "\n",
    "def load_data(ipath, epath):\n",
    "    images = sorted(glob(os.path.join(ipath)))\n",
    "    edges = sorted(glob(os.path.join(epath)))\n",
    "    return images, edges\n",
    "\n",
    "images, edges = load_data(train_path, edge_train_path)\n",
    "valimg, valedg = load_data(val_path, edge_val_path)\n",
    "print(len(images), len(valimg))\n",
    "\n",
    "def read_image(path, H=12, W=12):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W,H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_edge(path, H=12, W=12):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (W,H))\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def preprocess(x,y,H=12, W=12):\n",
    "\n",
    "    def f(x,y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "\n",
    "        x = read_image(x)\n",
    "        y = read_edge(y)\n",
    "        return x, y\n",
    "\n",
    "    images, edges = tf.numpy_function(f, [x, y], [tf.float32, tf.float32])\n",
    "    images.set_shape([H, W, 3])\n",
    "    edges.set_shape([H, W, 1])\n",
    "    return images, edges\n",
    "\n",
    "def tf_data(x,y,bs):\n",
    "    data = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    data = data.shuffle(buffer_size=bs)\n",
    "    data = data.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    data = data.batch(12)\n",
    "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return data\n",
    "\n",
    "train_data = tf_data(images, edges, len(images))\n",
    "val_data = tf_data(valimg, valedg, len(valimg))\n",
    "\n",
    "print(len(train_data),len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7e5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model,model_name,train_data,test_data,lr,beta1,max_epochs,batch_size):\n",
    "    # Validation and Train dataset generation\n",
    "\n",
    "    train_data = train_data\n",
    "    n_train =len(train_data) #data_cache[\"n_files\"]\n",
    "    val_data = test_data\n",
    "    \n",
    "    # Summary and checkpoint manager\n",
    "    model_dir = model_name\n",
    "    summary_dir = os.path.join(output_path,'logs',model_dir)\n",
    "    train_log_dir=os.path.join(summary_dir,'train')\n",
    "    val_log_dir =os.path.join(summary_dir,'test')\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_path,\"checkpoint_dir\",model_dir)\n",
    "    epoch_ckpt_dir = checkpoint_dir + 'epochs'\n",
    "    os.makedirs(epoch_ckpt_dir, exist_ok=True)\n",
    "    os.makedirs(train_log_dir,exist_ok=True)\n",
    "    os.makedirs(val_log_dir,exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    val_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "    my_model = model\n",
    "\n",
    "    # accuracy = metrics.SparseCategoricalAccuracy()\n",
    "    accuracy = metrics.BinaryAccuracy()\n",
    "    accuracy_val = metrics.BinaryAccuracy()\n",
    "    loss_bc = losses.BinaryCrossentropy()\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=lr, beta_1=beta1)\n",
    "    iter = 0\n",
    "\n",
    "    imgs_res_folder = os.path.join(output_path,model_dir, \"current_training\")\n",
    "    os.makedirs(imgs_res_folder, exist_ok=True)\n",
    "    global_loss = 1000.\n",
    "    t_loss = []\n",
    "    ckpt_save_mode = \"h5\"\n",
    "    tmp_lr = lr\n",
    "    for epoch in range(max_epochs):\n",
    "        # training\n",
    "        t_loss = []\n",
    "        # if epoch in self.args.adjust_lr:\n",
    "        tmp_lr=tmp_lr*0.1\n",
    "        optimizer.lr.assign(tmp_lr)\n",
    "        for step, (x, y) in enumerate(train_data):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = my_model(x, training=True)\n",
    "\n",
    "                preds, loss = pre_process_binary_cross_entropy(\n",
    "                    loss_bc, pred, y, use_tf_loss=False)\n",
    "\n",
    "            accuracy.update_state(y_true=y, y_pred=preds[-1])\n",
    "            gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "\n",
    "            # logging the current accuracy value so far.\n",
    "            t_loss.append(loss.numpy())\n",
    "            if step % 10 == 0:\n",
    "                print(\"Epoch:\", epoch, \"Step:\", step, \"Loss: %.4f\" % loss.numpy(),\n",
    "                        \"Accuracy: %.4f\" % accuracy.result(), time.ctime())\n",
    "\n",
    "            #if step % 10 == 0:\n",
    "            #    # visualize preds\n",
    "            #    img_test = 'Epoch: {0} Sample {1}/{2} Loss: {3}' \\\n",
    "            #        .format(epoch, step, n_train // batch_size, loss.numpy())\n",
    "            #    vis_imgs = visualize_result(\n",
    "            #        x=x[2], y=y[2], p=preds, img_title=img_test)\n",
    "            #    cv.imwrite(os.path.join(imgs_res_folder, 'results.png'), vis_imgs)\n",
    "            if step % 20 == 0 and loss < global_loss:  # 500\n",
    "                if epoch==0 and step==0:\n",
    "                    tmp_loss = np.array(t_loss)\n",
    "                    with train_writer.as_default():\n",
    "                        tf.summary.scalar('loss', tmp_loss.mean(), step=epoch)\n",
    "                        tf.summary.scalar('accuracy', accuracy.result(), step=epoch)\n",
    "\n",
    "                save_ckpt_path = os.path.join(checkpoint_dir, \"DexiNedL_model.h5\")\n",
    "                Model.save_weights(my_model, save_ckpt_path, save_format='h5')\n",
    "\n",
    "                global_loss = loss\n",
    "                print(\"Model saved in:  \", save_ckpt_path, \"Current loss:\", global_loss.numpy())\n",
    "\n",
    "            iter += 1  # global iteration\n",
    "\n",
    "        t_loss = np.array(t_loss)\n",
    "        # train summary\n",
    "        if epoch!=0:\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('loss', t_loss.mean(), step=epoch)\n",
    "                tf.summary.scalar('accuracy', accuracy.result(), step=epoch)\n",
    "\n",
    "        Model.save_weights(my_model, os.path.join(epoch_ckpt_dir, \"DexiNed{}_model.h5\".format(str(epoch))),\n",
    "                            save_format=ckpt_save_mode)\n",
    "        print(\"Epoch:\", epoch, \"Model saved in Loss: \", t_loss.mean())\n",
    "\n",
    "        # validation\n",
    "        t_val_loss = []\n",
    "        for i, (x_val, y_val) in enumerate(val_data):\n",
    "\n",
    "            pred_val = my_model(x_val)\n",
    "            v_logits, V_loss = pre_process_binary_cross_entropy(\n",
    "                loss_bc, pred_val, y_val, use_tf_loss=False)\n",
    "            accuracy_val.update_state(y_true=y_val, y_pred=v_logits[-1])\n",
    "            t_val_loss.append(V_loss.numpy())\n",
    "            if i == 7:\n",
    "                break\n",
    "        val_acc = accuracy_val.result()\n",
    "        t_val_loss = np.array(t_val_loss)\n",
    "        print(\"Epoch(validation):\", epoch, \"Val loss: \", t_val_loss.mean(),\n",
    "                \"Accuracy: \", val_acc.numpy())\n",
    "        # validation summary\n",
    "        with val_writer.as_default():\n",
    "            tf.summary.scalar('loss', t_val_loss.mean(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', val_acc.numpy(), step=epoch)\n",
    "\n",
    "        # Reset metrics every epoch\n",
    "        accuracy.reset_states()\n",
    "        accuracy_val.reset_states()\n",
    "\n",
    "    my_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0323b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def smish(input):\n",
    "    return input * tf.math.tanh(tf.math.log(1+tf.math.sigmoid(input)))\n",
    "\n",
    "class Smish(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Init method.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the function.\n",
    "        \"\"\"\n",
    "        return smish(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weight_init = tf.initializers.glorot_uniform()\n",
    "\n",
    "l2 = regularizers.l2\n",
    "w_decay=1e-3\n",
    "\n",
    "glorot_normal = RandomNormal(stddev=0.01)\n",
    "\n",
    "orthogonal = tf.keras.initializers.Orthogonal(\n",
    "    gain=1.0, seed=None\n",
    ")\n",
    "\n",
    "self_conv2D_1 = Conv2D(16 , kernel_size=(3,3),strides=(2,2),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_conv2D_11 = Conv2D(16 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_conv2D_17 = Conv2D(32 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_conv2D_18 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',activation='relu',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_conv2D_19 = Conv2D(32 , kernel_size=(1,1),strides=(2,2),padding = 'same')\n",
    "self_conv2D_12 = Conv2D(32 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_transpoze2D_1 = Conv2DTranspose(1 , kernel_size=(2,2),strides=(2,2),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_conv2D_5 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',activation='relu',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_maxpool2D_1 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_conv2D_24 = Conv2D(48 , kernel_size=(1,1),strides=(1,1),padding = 'same')\n",
    "self_transpoze2D_4 = Conv2DTranspose(1 , kernel_size=(2,2),strides=(2,2),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_conv2D_2 = Conv2D(32 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_conv2D_3 = Conv2D(48 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_conv2D_20 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',activation='relu',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_transpoze2D_2 = Conv2DTranspose(16 , kernel_size=(2,2),strides=(2,2),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_conv2D_21 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',activation='relu',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_transpoze2D_3 = Conv2DTranspose(1 , kernel_size=(4,4),strides=(2,2),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_concatenate_1 = Concatenate(axis=3)\n",
    "self_conv2D_22 = Conv2D(24 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_conv2D_23 = Conv2D(24 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_conv2D_6 = Conv2D(24 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu')\n",
    "self_concatenate_2 = Concatenate(axis=3)\n",
    "self_conv2D_7 = Conv2D(24 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu')\n",
    "self_concatenate_3 = Concatenate(axis=3)\n",
    "self_conv2D_8 = Conv2D(24 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu')\n",
    "self_concatenate_4 = Concatenate(axis=3)\n",
    "self_conv2D_4 = Conv2D(24 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu')\n",
    "self_conv2D_10 = Conv2D(32 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu',kernel_initializer= orthogonal)\n",
    "self_conv2D_13 = Conv2D(32 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu')\n",
    "self_concatenate_5 = Concatenate(axis=3)\n",
    "self_conv2D_14 = Conv2D(32 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu')\n",
    "self_concatenate_6 = Concatenate(axis=3)\n",
    "self_conv2D_15 = Conv2D(32 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu')\n",
    "self_concatenate_7 = Concatenate(axis=3)\n",
    "self_conv2D_16 = Conv2D(32 , kernel_size=(3,3),strides=(1,1),padding = 'same',activation='relu')\n",
    "self_conv2D_9 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',activation='relu',kernel_initializer= orthogonal)\n",
    "\n",
    "self_activation_9=Smish()\n",
    "self_activation_2=Smish()\n",
    "self_activation_3=Smish()\n",
    "self_activation_8=Smish()\n",
    "self_activation_7=Smish()\n",
    "self_activation_5=Smish()\n",
    "self_activation_1=Smish()\n",
    "self_activation_6=Smish()\n",
    "self_activation_4=Smish()\n",
    "\n",
    "x = Input(shape=(12,12,3))\n",
    "\n",
    "\n",
    "conv2D_1 = self_conv2D_1(x)\n",
    "\n",
    "\n",
    "activation_9 = self_activation_9(conv2D_1)\n",
    "conv2D_11 = self_conv2D_11(activation_9)\n",
    "activation_2 = self_activation_2(conv2D_11)\n",
    "conv2D_17 = self_conv2D_17(activation_2)\n",
    "conv2D_18 = self_conv2D_18(activation_2)\n",
    "conv2D_19 = self_conv2D_19(activation_2)\n",
    "activation_3 = self_activation_3(conv2D_17)\n",
    "activation_8 = self_activation_8(conv2D_18)\n",
    "conv2D_12 = self_conv2D_12(activation_3)\n",
    "transpoze2D_1 = self_transpoze2D_1(activation_8)\n",
    "conv2D_5 = self_conv2D_5(conv2D_12)\n",
    "maxpool2D_1 = self_maxpool2D_1(conv2D_12)\n",
    "activation_7 = self_activation_7(conv2D_5)\n",
    "add_1 = Add()([conv2D_19,maxpool2D_1])\n",
    "conv2D_24 = self_conv2D_24(maxpool2D_1)\n",
    "transpoze2D_4 = self_transpoze2D_4(activation_7)\n",
    "conv2D_2 = self_conv2D_2(add_1)\n",
    "activation_5 = self_activation_5(conv2D_2)\n",
    "conv2D_3 = self_conv2D_3(activation_5)\n",
    "average_1 = Average()([conv2D_3,conv2D_24])\n",
    "conv2D_20 = self_conv2D_20(average_1)\n",
    "activation_1 = self_activation_1(conv2D_20)\n",
    "transpoze2D_2 = self_transpoze2D_2(activation_1)\n",
    "conv2D_21 = self_conv2D_21(transpoze2D_2)\n",
    "activation_6 = self_activation_6(conv2D_21)\n",
    "transpoze2D_3 = self_transpoze2D_3(activation_6)\n",
    "concatenate_1_TMP = [transpoze2D_1,transpoze2D_3,transpoze2D_4]\n",
    "concatenate_1 = self_concatenate_1(concatenate_1_TMP)\n",
    "conv2D_22 = self_conv2D_22(concatenate_1)\n",
    "conv2D_23 = self_conv2D_23(conv2D_22)\n",
    "activation_4 = self_activation_4(conv2D_23)\n",
    "conv2D_6 = self_conv2D_6(activation_4)\n",
    "concatenate_2_TMP = [conv2D_23,conv2D_6]\n",
    "concatenate_2 = self_concatenate_2(concatenate_2_TMP)\n",
    "conv2D_7 = self_conv2D_7(concatenate_2)\n",
    "concatenate_3_TMP = [concatenate_2,conv2D_7]\n",
    "concatenate_3 = self_concatenate_3(concatenate_3_TMP)\n",
    "conv2D_8 = self_conv2D_8(concatenate_3)\n",
    "concatenate_4_TMP = [concatenate_2,concatenate_3,conv2D_8]\n",
    "concatenate_4 = self_concatenate_4(concatenate_4_TMP)\n",
    "conv2D_4 = self_conv2D_4(concatenate_4)\n",
    "\n",
    "\n",
    "add_2 = Add()([conv2D_23,conv2D_4])\n",
    "\n",
    "\n",
    "conv2D_10 = self_conv2D_10(add_2)\n",
    "conv2D_13 = self_conv2D_13(conv2D_10)\n",
    "concatenate_5_TMP = [conv2D_10,conv2D_13]\n",
    "concatenate_5 = self_concatenate_5(concatenate_5_TMP)\n",
    "conv2D_14 = self_conv2D_14(concatenate_5)\n",
    "concatenate_6_TMP = [concatenate_5,conv2D_14]\n",
    "concatenate_6 = self_concatenate_6(concatenate_6_TMP)\n",
    "conv2D_15 = self_conv2D_15(concatenate_6)\n",
    "concatenate_7_TMP = [concatenate_5,concatenate_6,conv2D_15]\n",
    "concatenate_7 = self_concatenate_7(concatenate_7_TMP)\n",
    "conv2D_16 = self_conv2D_16(concatenate_7)\n",
    "add_3 = Add()([conv2D_10,conv2D_16])\n",
    "conv2D_9 = self_conv2D_9(add_3)\n",
    "\n",
    "#outputs = tf.nn.depth_to_space(conv2D_9, 1)\n",
    "model = keras.Model(inputs =x , outputs=conv2D_9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef598a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_binary_cross_entropy(bc_loss,input, label, use_tf_loss=False):\n",
    "    # preprocess data\n",
    "    y = label\n",
    "    loss = 0\n",
    "    w_loss=1.0\n",
    "    preds = []\n",
    "    \n",
    "    #print(label.shape)\n",
    "    for tmp_p in input:\n",
    "        # tmp_p = input[i]\n",
    "\n",
    "        # loss processing\n",
    "        tmp_y = tf.cast(y, dtype=tf.float32)\n",
    "        mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "        b,h,w,c=mask.get_shape()\n",
    "        positives = tf.math.reduce_sum(mask, axis=[1, 2, 3], keepdims=True)\n",
    "        negatives = h*w*c-positives\n",
    "\n",
    "        beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "        beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "        pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "        logits = tf.sigmoid(tmp_p)\n",
    "\n",
    "        l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "                         sample_weight=pos_w)\n",
    "\n",
    "        preds.append(logits)\n",
    "        loss += (l_cost*w_loss)\n",
    "\n",
    "    return preds, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee83c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = DataLoader(data_path, \"train_list.txt\", \"blender1\", 12, 12, 4, True)\n",
    "# val_data = DataLoader(data_path, \"val_list.txt\", \"blender2\", 12, 12, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", run_eagerly=True)\n",
    "\n",
    "train(model,\"dexined_try\",train_data,val_data,0.0001,0.5,11,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bf2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
