{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401b9df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 23:04:18.148483: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-15 23:04:18.148604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-15 23:04:18.254344: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import optimizers, metrics, losses\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,Conv1DTranspose,AveragePooling1D,GlobalMaxPool1D,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Conv2DTranspose,AveragePooling2D,GlobalMaxPool2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv3D,MaxPooling3D,Conv3DTranspose,AveragePooling3D,GlobalMaxPool3D,GlobalAveragePooling3D\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout,Concatenate,Layer,BatchNormalization,Input,Add,Activation,Average\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7803cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/Tomasz/4T/work/tmp/tinyDexinedDataset2/test_edge/*\n",
      "4 2\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "#data_path = \"outlineOutput\"\n",
    "#data_path = \"H:\\\\download\\\\blender\\\\projects\\\\vdmTests\\\\outlineOutput\"\n",
    "#output_path = \"H:\\\\tmp\\\\dexined\"\n",
    "\n",
    "data_path = \"/media/Tomasz/4T/work/tmp/tinyDexinedDataset2\"\n",
    "output_path = \"/media/Tomasz/4T/work/tmp/teed\"\n",
    "\n",
    "train_path = data_path + \"/train/*\"\n",
    "edge_train_path = data_path + \"/train_edge/*\"\n",
    "\n",
    "test_path = data_path + \"/test/*\"\n",
    "edge_test_path = data_path + \"/test_edge/*\"\n",
    "\n",
    "val_path = data_path + \"/val/*\"\n",
    "edge_val_path = data_path + \"/val_edge/*\"\n",
    "\n",
    "print(edge_test_path)\n",
    "\n",
    "def load_data(ipath, epath):\n",
    "    images = sorted(glob(os.path.join(ipath)))\n",
    "    edges = sorted(glob(os.path.join(epath)))\n",
    "    return images, edges\n",
    "\n",
    "images, edges = load_data(train_path, edge_train_path)\n",
    "valimg, valedg = load_data(val_path, edge_val_path)\n",
    "print(len(images), len(valimg))\n",
    "\n",
    "def read_image(path, H=512, W=912):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W,H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_edge(path, H=512, W=912):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (W,H))\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def preprocess(x,y,H=512, W=912):\n",
    "\n",
    "    def f(x,y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "\n",
    "        x = read_image(x)\n",
    "        y = read_edge(y)\n",
    "        return x, y\n",
    "\n",
    "    images, edges = tf.numpy_function(f, [x, y], [tf.float32, tf.float32])\n",
    "    images.set_shape([H, W, 3])\n",
    "    edges.set_shape([H, W, 1])\n",
    "    return images, edges\n",
    "\n",
    "def tf_data(x,y,bs):\n",
    "    data = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    data = data.shuffle(buffer_size=bs)\n",
    "    data = data.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    data = data.batch(2)\n",
    "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return data\n",
    "\n",
    "train_data = tf_data(images, edges, len(images))\n",
    "val_data = tf_data(valimg, valedg, len(valimg))\n",
    "\n",
    "print(len(train_data),len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1c5a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model,model_name,train_data,test_data,lr,beta1,max_epochs,batch_size):\n",
    "    # Validation and Train dataset generation\n",
    "\n",
    "    train_data = train_data\n",
    "    n_train =len(train_data) #data_cache[\"n_files\"]\n",
    "    val_data = test_data\n",
    "    \n",
    "    # Summary and checkpoint manager\n",
    "    model_dir = model_name\n",
    "    summary_dir = os.path.join(output_path,'logs',model_dir)\n",
    "    train_log_dir=os.path.join(summary_dir,'train')\n",
    "    val_log_dir =os.path.join(summary_dir,'test')\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_path,\"checkpoint_dir\",model_dir)\n",
    "    epoch_ckpt_dir = checkpoint_dir + 'epochs'\n",
    "    os.makedirs(epoch_ckpt_dir, exist_ok=True)\n",
    "    os.makedirs(train_log_dir,exist_ok=True)\n",
    "    os.makedirs(val_log_dir,exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    val_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "    my_model = model\n",
    "\n",
    "    # accuracy = metrics.SparseCategoricalAccuracy()\n",
    "    accuracy = metrics.BinaryAccuracy()\n",
    "    accuracy_val = metrics.BinaryAccuracy()\n",
    "    loss_bc = losses.BinaryCrossentropy()\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=lr, beta_1=beta1)\n",
    "    iter = 0\n",
    "\n",
    "    imgs_res_folder = os.path.join(output_path,model_dir, \"current_training\")\n",
    "    os.makedirs(imgs_res_folder, exist_ok=True)\n",
    "    global_loss = 1000.\n",
    "    t_loss = []\n",
    "    ckpt_save_mode = \"h5\"\n",
    "    tmp_lr = lr\n",
    "    for epoch in range(max_epochs):\n",
    "        # training\n",
    "        t_loss = []\n",
    "        # if epoch in self.args.adjust_lr:\n",
    "        tmp_lr=tmp_lr*0.1\n",
    "        optimizer.lr.assign(tmp_lr)\n",
    "        for step, (x, y) in enumerate(train_data):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = my_model(x, training=True)\n",
    "\n",
    "                preds, loss = pre_process_binary_cross_entropy(\n",
    "                    loss_bc, pred, y, use_tf_loss=False)\n",
    "\n",
    "            accuracy.update_state(y_true=y, y_pred=preds[-1])\n",
    "            gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "\n",
    "            # logging the current accuracy value so far.\n",
    "            t_loss.append(loss.numpy())\n",
    "            if step % 10 == 0:\n",
    "                print(\"Epoch:\", epoch, \"Step:\", step, \"Loss: %.4f\" % loss.numpy(),\n",
    "                        \"Accuracy: %.4f\" % accuracy.result(), time.ctime())\n",
    "\n",
    "            #if step % 10 == 0:\n",
    "            #    # visualize preds\n",
    "            #    img_test = 'Epoch: {0} Sample {1}/{2} Loss: {3}' \\\n",
    "            #        .format(epoch, step, n_train // batch_size, loss.numpy())\n",
    "            #    vis_imgs = visualize_result(\n",
    "            #        x=x[2], y=y[2], p=preds, img_title=img_test)\n",
    "            #    cv.imwrite(os.path.join(imgs_res_folder, 'results.png'), vis_imgs)\n",
    "            if step % 20 == 0 and loss < global_loss:  # 500\n",
    "                if epoch==0 and step==0:\n",
    "                    tmp_loss = np.array(t_loss)\n",
    "                    with train_writer.as_default():\n",
    "                        tf.summary.scalar('loss', tmp_loss.mean(), step=epoch)\n",
    "                        tf.summary.scalar('accuracy', accuracy.result(), step=epoch)\n",
    "\n",
    "                save_ckpt_path = os.path.join(checkpoint_dir, \"DexiNedL_model.h5\")\n",
    "                Model.save_weights(my_model, save_ckpt_path, save_format='h5')\n",
    "\n",
    "                global_loss = loss\n",
    "                print(\"Model saved in:  \", save_ckpt_path, \"Current loss:\", global_loss.numpy())\n",
    "\n",
    "            iter += 1  # global iteration\n",
    "\n",
    "        t_loss = np.array(t_loss)\n",
    "        # train summary\n",
    "        if epoch!=0:\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('loss', t_loss.mean(), step=epoch)\n",
    "                tf.summary.scalar('accuracy', accuracy.result(), step=epoch)\n",
    "\n",
    "        Model.save_weights(my_model, os.path.join(epoch_ckpt_dir, \"DexiNed{}_model.h5\".format(str(epoch))),\n",
    "                            save_format=ckpt_save_mode)\n",
    "        print(\"Epoch:\", epoch, \"Model saved in Loss: \", t_loss.mean())\n",
    "\n",
    "        # validation\n",
    "        t_val_loss = []\n",
    "        for i, (x_val, y_val) in enumerate(val_data):\n",
    "\n",
    "            pred_val = my_model(x_val)\n",
    "            v_logits, V_loss = pre_process_binary_cross_entropy(\n",
    "                loss_bc, pred_val, y_val, use_tf_loss=False)\n",
    "            accuracy_val.update_state(y_true=y_val, y_pred=v_logits[-1])\n",
    "            t_val_loss.append(V_loss.numpy())\n",
    "            if i == 7:\n",
    "                break\n",
    "        val_acc = accuracy_val.result()\n",
    "        t_val_loss = np.array(t_val_loss)\n",
    "        print(\"Epoch(validation):\", epoch, \"Val loss: \", t_val_loss.mean(),\n",
    "                \"Accuracy: \", val_acc.numpy())\n",
    "        # validation summary\n",
    "        with val_writer.as_default():\n",
    "            tf.summary.scalar('loss', t_val_loss.mean(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', val_acc.numpy(), step=epoch)\n",
    "\n",
    "        # Reset metrics every epoch\n",
    "        accuracy.reset_states()\n",
    "        accuracy_val.reset_states()\n",
    "\n",
    "    #my_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48c53b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\download\\anaconda3_envs\\generativeTensorflowCudaV2\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weight_init = tf.initializers.glorot_uniform()\n",
    "\n",
    "l2 = regularizers.l2\n",
    "w_decay=1e-3\n",
    "\n",
    "glorot_normal = RandomNormal(stddev=0.01)\n",
    "\n",
    "orthogonal = tf.keras.initializers.Orthogonal(\n",
    "    gain=1.0, seed=None\n",
    ")\n",
    "\n",
    "self_conv2D_3 = Conv2D(32 , kernel_size=(3,3),strides=(2,2),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_batchnormalization_24 = BatchNormalization()\n",
    "self_conv2D_40 = Conv2D(64 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_batchnormalization_18 = BatchNormalization()\n",
    "self_activation_11 = Activation(activation='relu')\n",
    "self_conv2D_1 = Conv2D(128 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_conv2D_16 = Conv2D(128 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_51 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_1 = BatchNormalization()\n",
    "self_transpoze2D_9 = Conv2DTranspose(1 , kernel_size=(2,2),strides=(2,2),padding = 'same')\n",
    "self_conv2D_49 = Conv2D(128 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_8 = BatchNormalization()\n",
    "self_activation_1 = Activation(activation='relu')\n",
    "self_conv2D_17 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_3 = Conv2D(32 , kernel_size=(3,3),strides=(2,2),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_batchnormalization_24 = BatchNormalization()\n",
    "self_conv2D_40 = Conv2D(64 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_batchnormalization_18 = BatchNormalization()\n",
    "self_activation_11 = Activation(activation='relu')\n",
    "self_conv2D_1 = Conv2D(128 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_conv2D_16 = Conv2D(128 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_51 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_1 = BatchNormalization()\n",
    "self_transpoze2D_9 = Conv2DTranspose(1 , kernel_size=(2,2),strides=(2,2),padding = 'same')\n",
    "self_conv2D_49 = Conv2D(128 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_8 = BatchNormalization()\n",
    "self_activation_1 = Activation(activation='relu')\n",
    "self_conv2D_17 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_maxpool2D_1 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_transpoze2D_10 = Conv2DTranspose(1 , kernel_size=(2,2),strides=(2,2),padding = 'same')\n",
    "self_conv2D_28 = Conv2D(256 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_36 = Conv2D(256 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_activation_8 = Activation(activation='relu')\n",
    "self_conv2D_47 = Conv2D(256 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_48 = Conv2D(512 , kernel_size=(1,1),strides=(2,2),padding = 'same')\n",
    "self_conv2D_33 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_6 = BatchNormalization()\n",
    "self_activation_9 = Activation(activation='relu')\n",
    "self_conv2D_31 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_22 = BatchNormalization()\n",
    "self_activation_6 = Activation(activation='relu')\n",
    "self_conv2D_30 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_4 = BatchNormalization()\n",
    "self_activation_7 = Activation(activation='relu')\n",
    "self_conv2D_18 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_20 = BatchNormalization()\n",
    "self_conv2D_38 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_maxpool2D_2 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_transpoze2D_15 = Conv2DTranspose(16 , kernel_size=(4,4),strides=(2,2),padding = 'same')\n",
    "self_conv2D_11 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_34 = Conv2D(512 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_50 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_2 = Conv2DTranspose(1 , kernel_size=(4,4),strides=(2,2),padding = 'same')\n",
    "self_activation_10 = Activation(activation='relu')\n",
    "self_conv2D_32 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_10 = BatchNormalization()\n",
    "self_activation_5 = Activation(activation='relu')\n",
    "self_conv2D_27 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_19 = BatchNormalization()\n",
    "self_activation_24 = Activation(activation='relu')\n",
    "self_conv2D_26 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_13 = BatchNormalization()\n",
    "self_activation_16 = Activation(activation='relu')\n",
    "self_conv2D_13 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_17 = BatchNormalization()\n",
    "self_activation_22 = Activation(activation='relu')\n",
    "self_conv2D_29 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_15 = BatchNormalization()\n",
    "self_activation_23 = Activation(activation='relu')\n",
    "self_conv2D_24 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_12 = BatchNormalization()\n",
    "self_conv2D_44 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_maxpool2D_3 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_transpoze2D_7 = Conv2DTranspose(16 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_conv2D_45 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_activation_15 = Activation(activation='relu')\n",
    "self_conv2D_6 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_2 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_8 = Conv2DTranspose(16 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_conv2D_43 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_41 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_7 = BatchNormalization()\n",
    "self_transpoze2D_14 = Conv2DTranspose(1 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_activation_17 = Activation(activation='relu')\n",
    "self_conv2D_19 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_16 = BatchNormalization()\n",
    "self_activation_13 = Activation(activation='relu')\n",
    "self_conv2D_21 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_14 = BatchNormalization()\n",
    "self_activation_14 = Activation(activation='relu')\n",
    "self_conv2D_15 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_5 = BatchNormalization()\n",
    "self_activation_18 = Activation(activation='relu')\n",
    "self_conv2D_14 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_26 = BatchNormalization()\n",
    "self_activation_19 = Activation(activation='relu')\n",
    "self_conv2D_5 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_3 = BatchNormalization()\n",
    "self_conv2D_8 = Conv2D(256 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_35 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_activation_20 = Activation(activation='relu')\n",
    "self_transpoze2D_4 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_22 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_12 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_11 = BatchNormalization()\n",
    "self_transpoze2D_12 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_activation_21 = Activation(activation='relu')\n",
    "self_conv2D_37 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_10 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_5 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_batchnormalization_21 = BatchNormalization()\n",
    "self_conv2D_42 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_13 = Conv2DTranspose(1 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_activation_3 = Activation(activation='relu')\n",
    "self_conv2D_20 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_9 = BatchNormalization()\n",
    "self_activation_4 = Activation(activation='relu')\n",
    "self_conv2D_23 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_2 = BatchNormalization()\n",
    "self_activation_2 = Activation(activation='relu')\n",
    "self_conv2D_7 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_23 = BatchNormalization()\n",
    "self_activation_12 = Activation(activation='relu')\n",
    "self_conv2D_25 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_25 = BatchNormalization()\n",
    "self_conv2D_39 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_11 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_4 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_3 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_9 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_6 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_46 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_1 = Conv2DTranspose(1 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "#self_block_cat = SingleConvBlock(\n",
    "#    1,k_size=(1,1),stride=(1,1),\n",
    "#    w_init=tf.constant_initializer(1/5))\n",
    "self_transpoze2D_10 = Conv2DTranspose(1 , kernel_size=(2,2),strides=(2,2),padding = 'same')\n",
    "self_conv2D_28 = Conv2D(256 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_36 = Conv2D(256 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_activation_8 = Activation(activation='relu')\n",
    "self_conv2D_47 = Conv2D(256 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_48 = Conv2D(512 , kernel_size=(1,1),strides=(2,2),padding = 'same')\n",
    "self_conv2D_33 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_6 = BatchNormalization()\n",
    "self_activation_9 = Activation(activation='relu')\n",
    "self_conv2D_31 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_22 = BatchNormalization()\n",
    "self_activation_6 = Activation(activation='relu')\n",
    "self_conv2D_30 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_4 = BatchNormalization()\n",
    "self_activation_7 = Activation(activation='relu')\n",
    "self_conv2D_18 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_20 = BatchNormalization()\n",
    "self_conv2D_38 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_maxpool2D_2 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_transpoze2D_15 = Conv2DTranspose(16 , kernel_size=(4,4),strides=(2,2),padding = 'same')\n",
    "self_conv2D_11 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_34 = Conv2D(512 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_50 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_2 = Conv2DTranspose(1 , kernel_size=(4,4),strides=(2,2),padding = 'same')\n",
    "self_activation_10 = Activation(activation='relu')\n",
    "self_conv2D_32 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_10 = BatchNormalization()\n",
    "self_activation_5 = Activation(activation='relu')\n",
    "self_conv2D_27 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_19 = BatchNormalization()\n",
    "self_activation_24 = Activation(activation='relu')\n",
    "self_conv2D_26 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_13 = BatchNormalization()\n",
    "self_activation_16 = Activation(activation='relu')\n",
    "self_conv2D_13 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_17 = BatchNormalization()\n",
    "self_activation_22 = Activation(activation='relu')\n",
    "self_conv2D_29 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_15 = BatchNormalization()\n",
    "self_activation_23 = Activation(activation='relu')\n",
    "self_conv2D_24 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_12 = BatchNormalization()\n",
    "self_conv2D_44 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_maxpool2D_3 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_transpoze2D_7 = Conv2DTranspose(16 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_conv2D_45 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_activation_15 = Activation(activation='relu')\n",
    "self_conv2D_6 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_2 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_8 = Conv2DTranspose(16 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_conv2D_43 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_41 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_7 = BatchNormalization()\n",
    "self_transpoze2D_14 = Conv2DTranspose(1 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_activation_17 = Activation(activation='relu')\n",
    "self_conv2D_19 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_16 = BatchNormalization()\n",
    "self_activation_13 = Activation(activation='relu')\n",
    "self_conv2D_21 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_14 = BatchNormalization()\n",
    "self_activation_14 = Activation(activation='relu')\n",
    "self_conv2D_15 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_5 = BatchNormalization()\n",
    "self_activation_18 = Activation(activation='relu')\n",
    "self_conv2D_14 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_26 = BatchNormalization()\n",
    "self_activation_19 = Activation(activation='relu')\n",
    "self_conv2D_5 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_3 = BatchNormalization()\n",
    "self_conv2D_8 = Conv2D(256 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_35 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_activation_20 = Activation(activation='relu')\n",
    "self_transpoze2D_4 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_22 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_12 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_11 = BatchNormalization()\n",
    "self_transpoze2D_12 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_activation_21 = Activation(activation='relu')\n",
    "self_conv2D_37 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_10 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_5 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_batchnormalization_21 = BatchNormalization()\n",
    "self_conv2D_42 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_13 = Conv2DTranspose(1 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_activation_3 = Activation(activation='relu')\n",
    "self_conv2D_20 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_9 = BatchNormalization()\n",
    "self_activation_4 = Activation(activation='relu')\n",
    "self_conv2D_23 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_2 = BatchNormalization()\n",
    "self_activation_2 = Activation(activation='relu')\n",
    "self_conv2D_7 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_23 = BatchNormalization()\n",
    "self_activation_12 = Activation(activation='relu')\n",
    "self_conv2D_25 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_25 = BatchNormalization()\n",
    "self_conv2D_39 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_11 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_4 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_3 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_9 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_6 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_46 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_1 = Conv2DTranspose(1 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "#self_block_cat = SingleConvBlock(\n",
    "#    1,k_size=(1,1),stride=(1,1),\n",
    "#    w_init=tf.constant_initializer(1/5))\n",
    "\n",
    "self_last_conv = Conv2D(1, kernel_size=(1,1), strides=(1,1), padding = 'same', kernel_initializer=tf.constant_initializer(1/5))\n",
    "self_last_batchnormalization = BatchNormalization()\n",
    "self_last_activation = Activation(activation='relu')\n",
    "self_last_conv = Conv2D(1, kernel_size=(1,1), strides=(1,1), padding = 'same', kernel_initializer=tf.constant_initializer(1/5))\n",
    "self_last_batchnormalization = BatchNormalization()\n",
    "self_last_activation = Activation(activation='relu')\n",
    "\n",
    "#-------------------------------------\n",
    "#-------------------------------------\n",
    "\n",
    "x = Input(shape=(16,16,3))\n",
    "conv2D_3 = self_conv2D_3(x)\n",
    "batchnormalization_24 = self_batchnormalization_24(conv2D_3)\n",
    "conv2D_40 = self_conv2D_40(batchnormalization_24)\n",
    "batchnormalization_18 = self_batchnormalization_18(conv2D_40)\n",
    "activation_11 = self_activation_11(batchnormalization_18)\n",
    "conv2D_1 = self_conv2D_1(activation_11)\n",
    "conv2D_16 = self_conv2D_16(activation_11)\n",
    "conv2D_51 = self_conv2D_51(activation_11)\n",
    "batchnormalization_1 = self_batchnormalization_1(conv2D_1)\n",
    "transpoze2D_9 = self_transpoze2D_9(conv2D_51)\n",
    "conv2D_49 = self_conv2D_49(batchnormalization_1)\n",
    "batchnormalization_8 = self_batchnormalization_8(conv2D_49)\n",
    "activation_1 = self_activation_1(batchnormalization_8)\n",
    "conv2D_17 = self_conv2D_17(activation_1)\n",
    "maxpool2D_1 = self_maxpool2D_1(activation_1)\n",
    "transpoze2D_10 = self_transpoze2D_10(conv2D_17)\n",
    "add_1 = Add()([conv2D_16,maxpool2D_1])\n",
    "conv2D_28 = self_conv2D_28(maxpool2D_1)\n",
    "conv2D_36 = self_conv2D_36(maxpool2D_1)\n",
    "activation_8 = self_activation_8(add_1)\n",
    "conv2D_47 = self_conv2D_47(add_1)\n",
    "conv2D_48 = self_conv2D_48(conv2D_36)\n",
    "conv2D_33 = self_conv2D_33(activation_8)\n",
    "batchnormalization_6 = self_batchnormalization_6(conv2D_33)\n",
    "activation_9 = self_activation_9(batchnormalization_6)\n",
    "conv2D_31 = self_conv2D_31(activation_9)\n",
    "batchnormalization_22 = self_batchnormalization_22(conv2D_31)\n",
    "average_5 = Average()([batchnormalization_22,conv2D_28])\n",
    "activation_6 = self_activation_6(average_5)\n",
    "conv2D_30 = self_conv2D_30(activation_6)\n",
    "batchnormalization_4 = self_batchnormalization_4(conv2D_30)\n",
    "activation_7 = self_activation_7(batchnormalization_4)\n",
    "conv2D_18 = self_conv2D_18(activation_7)\n",
    "batchnormalization_20 = self_batchnormalization_20(conv2D_18)\n",
    "average_4 = Average()([batchnormalization_20,conv2D_28])\n",
    "conv2D_38 = self_conv2D_38(average_4)\n",
    "maxpool2D_2 = self_maxpool2D_2(average_4)\n",
    "transpoze2D_15 = self_transpoze2D_15(conv2D_38)\n",
    "add_2 = Add()([conv2D_47,maxpool2D_2])\n",
    "add_4 = Add()([conv2D_36,maxpool2D_2])\n",
    "conv2D_11 = self_conv2D_11(transpoze2D_15)\n",
    "conv2D_34 = self_conv2D_34(add_2)\n",
    "conv2D_50 = self_conv2D_50(add_4)\n",
    "transpoze2D_2 = self_transpoze2D_2(conv2D_11)\n",
    "activation_10 = self_activation_10(add_2)\n",
    "conv2D_32 = self_conv2D_32(activation_10)\n",
    "batchnormalization_10 = self_batchnormalization_10(conv2D_32)\n",
    "activation_5 = self_activation_5(batchnormalization_10)\n",
    "conv2D_27 = self_conv2D_27(activation_5)\n",
    "batchnormalization_19 = self_batchnormalization_19(conv2D_27)\n",
    "average_3 = Average()([batchnormalization_19,conv2D_50])\n",
    "activation_24 = self_activation_24(average_3)\n",
    "conv2D_26 = self_conv2D_26(activation_24)\n",
    "batchnormalization_13 = self_batchnormalization_13(conv2D_26)\n",
    "activation_16 = self_activation_16(batchnormalization_13)\n",
    "conv2D_13 = self_conv2D_13(activation_16)\n",
    "batchnormalization_17 = self_batchnormalization_17(conv2D_13)\n",
    "average_2 = Average()([batchnormalization_17,conv2D_50])\n",
    "activation_22 = self_activation_22(average_2)\n",
    "conv2D_29 = self_conv2D_29(activation_22)\n",
    "batchnormalization_15 = self_batchnormalization_15(conv2D_29)\n",
    "activation_23 = self_activation_23(batchnormalization_15)\n",
    "conv2D_24 = self_conv2D_24(activation_23)\n",
    "batchnormalization_12 = self_batchnormalization_12(conv2D_24)\n",
    "average_11 = Average()([batchnormalization_12,conv2D_50])\n",
    "conv2D_44 = self_conv2D_44(average_11)\n",
    "maxpool2D_3 = self_maxpool2D_3(average_11)\n",
    "transpoze2D_7 = self_transpoze2D_7(conv2D_44)\n",
    "add_3 = Add()([conv2D_34,maxpool2D_3])\n",
    "add_5 = Add()([conv2D_48,maxpool2D_3])\n",
    "conv2D_45 = self_conv2D_45(transpoze2D_7)\n",
    "activation_15 = self_activation_15(add_3)\n",
    "conv2D_6 = self_conv2D_6(add_3)\n",
    "conv2D_2 = self_conv2D_2(add_5)\n",
    "transpoze2D_8 = self_transpoze2D_8(conv2D_45)\n",
    "conv2D_43 = self_conv2D_43(activation_15)\n",
    "conv2D_41 = self_conv2D_41(transpoze2D_8)\n",
    "batchnormalization_7 = self_batchnormalization_7(conv2D_43)\n",
    "transpoze2D_14 = self_transpoze2D_14(conv2D_41)\n",
    "activation_17 = self_activation_17(batchnormalization_7)\n",
    "conv2D_19 = self_conv2D_19(activation_17)\n",
    "batchnormalization_16 = self_batchnormalization_16(conv2D_19)\n",
    "average_7 = Average()([batchnormalization_16,conv2D_2])\n",
    "activation_13 = self_activation_13(average_7)\n",
    "conv2D_21 = self_conv2D_21(activation_13)\n",
    "batchnormalization_14 = self_batchnormalization_14(conv2D_21)\n",
    "activation_14 = self_activation_14(batchnormalization_14)\n",
    "conv2D_15 = self_conv2D_15(activation_14)\n",
    "batchnormalization_5 = self_batchnormalization_5(conv2D_15)\n",
    "average_6 = Average()([batchnormalization_5,conv2D_2])\n",
    "activation_18 = self_activation_18(average_6)\n",
    "conv2D_14 = self_conv2D_14(activation_18)\n",
    "batchnormalization_26 = self_batchnormalization_26(conv2D_14)\n",
    "activation_19 = self_activation_19(batchnormalization_26)\n",
    "conv2D_5 = self_conv2D_5(activation_19)\n",
    "batchnormalization_3 = self_batchnormalization_3(conv2D_5)\n",
    "average_1 = Average()([batchnormalization_3,conv2D_2])\n",
    "add_6 = Add()([average_1,conv2D_6])\n",
    "conv2D_8 = self_conv2D_8(average_1)\n",
    "conv2D_35 = self_conv2D_35(average_1)\n",
    "activation_20 = self_activation_20(add_6)\n",
    "transpoze2D_4 = self_transpoze2D_4(conv2D_35)\n",
    "conv2D_22 = self_conv2D_22(activation_20)\n",
    "conv2D_12 = self_conv2D_12(transpoze2D_4)\n",
    "batchnormalization_11 = self_batchnormalization_11(conv2D_22)\n",
    "transpoze2D_12 = self_transpoze2D_12(conv2D_12)\n",
    "activation_21 = self_activation_21(batchnormalization_11)\n",
    "conv2D_37 = self_conv2D_37(transpoze2D_12)\n",
    "conv2D_10 = self_conv2D_10(activation_21)\n",
    "transpoze2D_5 = self_transpoze2D_5(conv2D_37)\n",
    "batchnormalization_21 = self_batchnormalization_21(conv2D_10)\n",
    "conv2D_42 = self_conv2D_42(transpoze2D_5)\n",
    "average_10 = Average()([batchnormalization_21,conv2D_8])\n",
    "transpoze2D_13 = self_transpoze2D_13(conv2D_42)\n",
    "activation_3 = self_activation_3(average_10)\n",
    "conv2D_20 = self_conv2D_20(activation_3)\n",
    "batchnormalization_9 = self_batchnormalization_9(conv2D_20)\n",
    "activation_4 = self_activation_4(batchnormalization_9)\n",
    "conv2D_23 = self_conv2D_23(activation_4)\n",
    "batchnormalization_2 = self_batchnormalization_2(conv2D_23)\n",
    "average_9 = Average()([batchnormalization_2,conv2D_8])\n",
    "activation_2 = self_activation_2(average_9)\n",
    "conv2D_7 = self_conv2D_7(activation_2)\n",
    "batchnormalization_23 = self_batchnormalization_23(conv2D_7)\n",
    "activation_12 = self_activation_12(batchnormalization_23)\n",
    "conv2D_25 = self_conv2D_25(activation_12)\n",
    "batchnormalization_25 = self_batchnormalization_25(conv2D_25)\n",
    "average_8 = Average()([batchnormalization_25,conv2D_8])\n",
    "conv2D_39 = self_conv2D_39(average_8)\n",
    "transpoze2D_11 = self_transpoze2D_11(conv2D_39)\n",
    "conv2D_4 = self_conv2D_4(transpoze2D_11)\n",
    "transpoze2D_3 = self_transpoze2D_3(conv2D_4)\n",
    "conv2D_9 = self_conv2D_9(transpoze2D_3)\n",
    "transpoze2D_6 = self_transpoze2D_6(conv2D_9)\n",
    "conv2D_46 = self_conv2D_46(transpoze2D_6)\n",
    "transpoze2D_1 = self_transpoze2D_1(conv2D_46)\n",
    "#tmp = [transpoze2D_1,transpoze2D_2,transpoze2D_9,transpoze2D_10,transpoze2D_13,transpoze2D_14]\n",
    "tmp = [transpoze2D_9,transpoze2D_10,transpoze2D_2,transpoze2D_14,transpoze2D_13,transpoze2D_1]\n",
    "\n",
    "#-----------\n",
    "#concatenate_1 = tf.concat(tmp,3)\n",
    "#-----------\n",
    "#concat_lambda = lambda xs: tf.concat(xs, axis=3)\n",
    "#concatenate_1 = tf.keras.layers.Lambda(concat_lambda)(tmp,3)\n",
    "#-----------\n",
    "self_concatenate_1 = Concatenate(axis=3)\n",
    "concatenate_1 = self_concatenate_1(tmp)\n",
    "#-----------\n",
    "#concatenate_1 = tf.concat(tmp,3)\n",
    "#-----------\n",
    "#concat_lambda = lambda xs: tf.concat(xs, axis=3)\n",
    "#concatenate_1 = tf.keras.layers.Lambda(concat_lambda)(tmp,3)\n",
    "#-----------\n",
    "self_concatenate_1 = Concatenate(axis=3)\n",
    "concatenate_1 = self_concatenate_1(tmp)\n",
    "\n",
    "#print(f\"concatenate_1 shape: {concatenate_1.shape}\")\n",
    "#return concatenate_1\n",
    "#print(f\"concatenate_1 shape: {concatenate_1.shape}\")\n",
    "#return concatenate_1\n",
    "\n",
    "#results = [transpoze2D_1,transpoze2D_2,transpoze2D_9,transpoze2D_10,transpoze2D_13,transpoze2D_14]\n",
    "#block_cat = tf.concat(results, 3)  # BxHxWX6\n",
    "#print(f\"CONCATENATE_1 shape: {block_cat.shape}\")\n",
    "#block_cat = self_block_cat(block_cat)  # BxHxWX1\n",
    "#results.append(block_cat)\n",
    "#results = [transpoze2D_1,transpoze2D_2,transpoze2D_9,transpoze2D_10,transpoze2D_13,transpoze2D_14]\n",
    "#block_cat = tf.concat(results, 3)  # BxHxWX6\n",
    "#print(f\"CONCATENATE_1 shape: {block_cat.shape}\")\n",
    "#block_cat = self_block_cat(block_cat)  # BxHxWX1\n",
    "#results.append(block_cat)\n",
    "\n",
    "\n",
    "last_conv =self_last_conv(concatenate_1)\n",
    "last_batchnormalization = self_last_batchnormalization(last_conv)\n",
    "last_activation = self_last_activation(last_batchnormalization)\n",
    "#concatenate_1 = Concatenate()([transpoze2D_1,transpoze2D_3,transpoze2D_4,transpoze2D_7,transpoze2D_8,transpoze2D_10])\n",
    "\n",
    "last_conv =self_last_conv(concatenate_1)\n",
    "last_batchnormalization = self_last_batchnormalization(last_conv)\n",
    "last_activation = self_last_activation(last_batchnormalization)\n",
    "#concatenate_1 = Concatenate()([transpoze2D_1,transpoze2D_3,transpoze2D_4,transpoze2D_7,transpoze2D_8,transpoze2D_10])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs =x , outputs=last_activation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef598a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_binary_cross_entropy1(bc_loss,input, label, use_tf_loss=False):\n",
    "    # preprocess data\n",
    "    print(input.shape,label.shape)\n",
    "    y = label\n",
    "    loss = 0\n",
    "    w_loss=1.0\n",
    "    preds = []\n",
    "    \n",
    "    #print(label.shape)\n",
    "    #for f, b in zip(foo, bar):\n",
    "    #print(f, b)\n",
    "    for (tmp_p,tmpy) in zip(input,y):\n",
    "        print(\">\",tmp_p.shape,tmpy.shape)\n",
    "        # tmp_p = input[i]\n",
    "\n",
    "        # loss processing\n",
    "        tmp_y = tf.cast(tmpy, dtype=tf.float32)\n",
    "        print(tmp_y.numpy())\n",
    "        mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "        h,w,c=mask.get_shape()\n",
    "        #plt.imshow(mask)\n",
    "        positives = tf.math.reduce_sum(mask, axis=[1,2], keepdims=True)\n",
    "        negatives = h*w-positives\n",
    "\n",
    "        beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "        beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "        pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "        logits = tf.sigmoid(tmp_p)\n",
    "        #print(tmp_y.shape,logits.shape)\n",
    "        l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "                         sample_weight=pos_w)\n",
    "\n",
    "        preds.append(logits)\n",
    "        loss += (l_cost*w_loss)\n",
    "\n",
    "    return preds, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaa0b65d-6113-4508-84ea-f63162cecb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_binary_cross_entropy2(bc_loss,input, label, use_tf_loss=False):\n",
    "    # preprocess data\n",
    "    y = label\n",
    "    loss = 0\n",
    "    w_loss=1.0\n",
    "    preds = []\n",
    "    #for tmp_p in input:\n",
    "    # tmp_p = input[i]\n",
    "    tmp_p = input\n",
    "    # loss processing\n",
    "    tmp_y = tf.cast(y, dtype=tf.float32)\n",
    "    mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "    b,h,w,c=mask.get_shape()\n",
    "    positives = tf.math.reduce_sum(mask, axis=[1, 2, 3], keepdims=True)\n",
    "    negatives = h*w*c-positives\n",
    "\n",
    "    beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "    beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "    pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "    logits = tf.sigmoid(tmp_p)\n",
    "    print(tmp_y.shape,logits.shape)\n",
    "    l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "                     sample_weight=pos_w)\n",
    "\n",
    "    preds.append(logits)\n",
    "    loss += (l_cost*w_loss)\n",
    "\n",
    "    return preds, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee83c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = DataLoader(data_path, \"train_list.txt\", \"blender1\", 12, 12, 4, True)\n",
    "# val_data = DataLoader(data_path, \"val_list.txt\", \"blender2\", 12, 12, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1dc59c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      2\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, run_eagerly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdexined_try\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, model_name, train_data, test_data, lr, beta1, max_epochs, batch_size)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     49\u001b[0m     pred \u001b[38;5;241m=\u001b[39m my_model(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 51\u001b[0m     preds, loss \u001b[38;5;241m=\u001b[39m \u001b[43mpre_process_binary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_bc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tf_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m accuracy\u001b[38;5;241m.\u001b[39mupdate_state(y_true\u001b[38;5;241m=\u001b[39my, y_pred\u001b[38;5;241m=\u001b[39mpreds[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     55\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, my_model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mpre_process_binary_cross_entropy\u001b[1;34m(bc_loss, input, label, use_tf_loss)\u001b[0m\n\u001b[0;32m     21\u001b[0m pos_w \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mwhere(tf\u001b[38;5;241m.\u001b[39mequal(y, \u001b[38;5;241m0.0\u001b[39m), beta2, beta)\n\u001b[0;32m     22\u001b[0m logits \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msigmoid(tmp_p)\n\u001b[1;32m---> 24\u001b[0m l_cost \u001b[38;5;241m=\u001b[39m \u001b[43mbc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(logits)\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (l_cost\u001b[38;5;241m*\u001b[39mw_loss)\n",
      "File \u001b[1;32mH:\\download\\anaconda3_envs\\generativeTensorflowCudaV2\\lib\\site-packages\\keras\\losses.py:152\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 152\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(losses)\n\u001b[0;32m    154\u001b[0m reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reduction()\n",
      "File \u001b[1;32mH:\\download\\anaconda3_envs\\generativeTensorflowCudaV2\\lib\\site-packages\\keras\\losses.py:272\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    265\u001b[0m     y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[0;32m    266\u001b[0m         y_pred, y_true\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[0;32m    271\u001b[0m )\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mH:\\download\\anaconda3_envs\\generativeTensorflowCudaV2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mH:\\download\\anaconda3_envs\\generativeTensorflowCudaV2\\lib\\site-packages\\keras\\losses.py:2162\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[0;32m   2155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m label_smoothing) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m label_smoothing\n\u001b[0;32m   2157\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39msmart_cond\u001b[38;5;241m.\u001b[39msmart_cond(\n\u001b[0;32m   2158\u001b[0m     label_smoothing, _smooth_labels, \u001b[38;5;28;01mlambda\u001b[39;00m: y_true\n\u001b[0;32m   2159\u001b[0m )\n\u001b[0;32m   2161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m-> 2162\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_crossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   2163\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   2164\u001b[0m )\n",
      "File \u001b[1;32mH:\\download\\anaconda3_envs\\generativeTensorflowCudaV2\\lib\\site-packages\\keras\\backend.py:5685\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m   5682\u001b[0m output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mclip_by_value(output, epsilon_, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon_)\n\u001b[0;32m   5684\u001b[0m \u001b[38;5;66;03m# Compute cross entropy from probabilities.\u001b[39;00m\n\u001b[1;32m-> 5685\u001b[0m bce \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5686\u001b[0m bce \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m target) \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m output \u001b[38;5;241m+\u001b[39m epsilon())\n\u001b[0;32m   5687\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mbce\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Mul]"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", run_eagerly=True)\n",
    "\n",
    "train(model,\"dexined_try\",train_data,val_data,0.0001,0.5,2,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bf2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (x, y) in enumerate(train_data):\n",
    "    plt.figure()\n",
    "    for (tmp_p,tmpy) in zip(x,y):\n",
    "        print(\">\",tmp_p.shape,tmpy.shape)\n",
    "        # tmp_p = input[i]\n",
    "\n",
    "        # loss processing\n",
    "        tmp_y = tf.cast(tmpy, dtype=tf.float32)\n",
    "        print(tmp_y.numpy())\n",
    "        plt.imshow(tmp_y)\n",
    "        mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "        h,w,c=mask.get_shape()\n",
    "        print(mask.numpy())\n",
    "        plt.imshow(mask)\n",
    "        positives = tf.math.reduce_sum(mask, axis=[1,2], keepdims=True)\n",
    "        negatives = h*w-positives\n",
    "        print(positives.numpy())\n",
    "        plt.imshow(positives)\n",
    "\n",
    "        #beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "        #beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "        #pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "        #logits = tf.sigmoid(tmp_p)\n",
    "        #print(tmp_y.shape,logits.shape)\n",
    "        #l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "        #                 sample_weight=pos_w)\n",
    "\n",
    "        #preds.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4e557-7bd6-4a6a-9d01-2e3d5543493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (x, y) in enumerate(train_data):\n",
    "    for tmp_p in x:\n",
    "        print(\">\",tmp_p.shape,y.shape)\n",
    "        # tmp_p = input[i]\n",
    "\n",
    "        # loss processing\n",
    "        tmp_y = tf.cast(y, dtype=tf.float32)\n",
    "        mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "        b,h,w,c=mask.get_shape()\n",
    "        #plt.imshow(mask)\n",
    "        positives = tf.math.reduce_sum(mask, axis=[1,2,3], keepdims=True)\n",
    "        negatives = h*w*c-positives\n",
    "        print(\"positives\",positives.shape)\n",
    "        print(positives)\n",
    "        print(\"negatives\",negatives.shape)\n",
    "        print(negatives)\n",
    "        beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "        beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "        pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "        logits = tf.sigmoid(tmp_p)\n",
    "        #print(tmp_y.shape,logits.shape)\n",
    "        #l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "        #                 sample_weight=pos_w)\n",
    "\n",
    "        #preds.append(logits)\n",
    "        #loss += (l_cost*w_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
