{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401b9df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 01:37:39.720593: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-15 01:37:39.720673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-15 01:37:39.722998: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras import optimizers, metrics, losses\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
    "from tensorflow.keras.layers import Conv1D,MaxPooling1D,Conv1DTranspose,AveragePooling1D,GlobalMaxPool1D,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Conv2DTranspose,AveragePooling2D,GlobalMaxPool2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Conv3D,MaxPooling3D,Conv3DTranspose,AveragePooling3D,GlobalMaxPool3D,GlobalAveragePooling3D\n",
    "from tensorflow.keras.layers import Dense,Flatten,Dropout,Concatenate,Layer,BatchNormalization,Input,Add,Activation,Average\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1226371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/Tomasz/4T/work/tmp/tinyDexinedDataset2/test_edge/*\n",
      "4 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 01:37:47.084128: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.097647: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.097891: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.100504: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.100728: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.100837: I external/local_xla/xla/stream_executor"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.198146: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.198404: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.198463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-15 01:37:47.198571: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:00:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-15 01:37:47.198636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 48140 MB memory:  -> device: 0, name: Orin, pci bus id: 0000:00:00.0, compute capability: 8.7\n"
     ]
    }
   ],
   "source": [
    "#data_path = \"outlineOutput\"\n",
    "#data_path = \"H:\\\\download\\\\blender\\\\projects\\\\vdmTests\\\\outlineOutput\"\n",
    "#output_path = \"H:\\\\tmp\\\\dexined\"\n",
    "\n",
    "data_path = \"/media/Tomasz/4T/work/tmp/tinyDexinedDataset2\"\n",
    "output_path = \"/media/Tomasz/4T/work/tmp/teed\"\n",
    "\n",
    "train_path = data_path + \"/train/*\"\n",
    "edge_train_path = data_path + \"/train_edge/*\"\n",
    "\n",
    "test_path = data_path + \"/test/*\"\n",
    "edge_test_path = data_path + \"/test_edge/*\"\n",
    "\n",
    "val_path = data_path + \"/val/*\"\n",
    "edge_val_path = data_path + \"/val_edge/*\"\n",
    "\n",
    "print(edge_test_path)\n",
    "\n",
    "def load_data(ipath, epath):\n",
    "    images = sorted(glob(os.path.join(ipath)))\n",
    "    edges = sorted(glob(os.path.join(epath)))\n",
    "    return images, edges\n",
    "\n",
    "images, edges = load_data(train_path, edge_train_path)\n",
    "valimg, valedg = load_data(val_path, edge_val_path)\n",
    "print(len(images), len(valimg))\n",
    "\n",
    "def read_image(path, H=16, W=16):\n",
    "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W,H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def read_edge(path, H=16, W=16):\n",
    "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (W,H))\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def preprocess(x,y,H=16, W=16):\n",
    "\n",
    "    def f(x,y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "\n",
    "        x = read_image(x)\n",
    "        y = read_edge(y)\n",
    "        return x, y\n",
    "\n",
    "    images, edges = tf.numpy_function(f, [x, y], [tf.float32, tf.float32])\n",
    "    images.set_shape([H, W, 3])\n",
    "    edges.set_shape([H, W, 1])\n",
    "    return images, edges\n",
    "\n",
    "def tf_data(x,y,bs):\n",
    "    data = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    data = data.shuffle(buffer_size=bs)\n",
    "    data = data.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    data = data.batch(2)\n",
    "    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return data\n",
    "\n",
    "train_data = tf_data(images, edges, len(images))\n",
    "val_data = tf_data(valimg, valedg, len(valimg))\n",
    "\n",
    "print(len(train_data),len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7e5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model,model_name,train_data,test_data,lr,beta1,max_epochs,batch_size):\n",
    "    # Validation and Train dataset generation\n",
    "\n",
    "    train_data = train_data\n",
    "    n_train =len(train_data) #data_cache[\"n_files\"]\n",
    "    val_data = test_data\n",
    "    \n",
    "    # Summary and checkpoint manager\n",
    "    model_dir = model_name\n",
    "    summary_dir = os.path.join(output_path,'logs',model_dir)\n",
    "    train_log_dir=os.path.join(summary_dir,'train')\n",
    "    val_log_dir =os.path.join(summary_dir,'test')\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_path,\"checkpoint_dir\",model_dir)\n",
    "    epoch_ckpt_dir = checkpoint_dir + 'epochs'\n",
    "    os.makedirs(epoch_ckpt_dir, exist_ok=True)\n",
    "    os.makedirs(train_log_dir,exist_ok=True)\n",
    "    os.makedirs(val_log_dir,exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    val_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "    my_model = model\n",
    "\n",
    "    # accuracy = metrics.SparseCategoricalAccuracy()\n",
    "    accuracy = metrics.BinaryAccuracy()\n",
    "    accuracy_val = metrics.BinaryAccuracy()\n",
    "    loss_bc = losses.BinaryCrossentropy()\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate=lr, beta_1=beta1)\n",
    "    iter = 0\n",
    "\n",
    "    imgs_res_folder = os.path.join(output_path,model_dir, \"current_training\")\n",
    "    os.makedirs(imgs_res_folder, exist_ok=True)\n",
    "    global_loss = 1000.\n",
    "    t_loss = []\n",
    "    ckpt_save_mode = \"h5\"\n",
    "    tmp_lr = lr\n",
    "    for epoch in range(max_epochs):\n",
    "        # training\n",
    "        t_loss = []\n",
    "        # if epoch in self.args.adjust_lr:\n",
    "        tmp_lr=tmp_lr*0.1\n",
    "        optimizer.lr.assign(tmp_lr)\n",
    "        for step, (x, y) in enumerate(train_data):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                pred = my_model(x, training=True)\n",
    "\n",
    "                preds, loss = pre_process_binary_cross_entropy(\n",
    "                    loss_bc, pred, y, use_tf_loss=False)\n",
    "\n",
    "            accuracy.update_state(y_true=y, y_pred=preds[-1])\n",
    "            gradients = tape.gradient(loss, my_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, my_model.trainable_variables))\n",
    "\n",
    "            # logging the current accuracy value so far.\n",
    "            t_loss.append(loss.numpy())\n",
    "            if step % 10 == 0:\n",
    "                print(\"Epoch:\", epoch, \"Step:\", step, \"Loss: %.4f\" % loss.numpy(),\n",
    "                        \"Accuracy: %.4f\" % accuracy.result(), time.ctime())\n",
    "\n",
    "            #if step % 10 == 0:\n",
    "            #    # visualize preds\n",
    "            #    img_test = 'Epoch: {0} Sample {1}/{2} Loss: {3}' \\\n",
    "            #        .format(epoch, step, n_train // batch_size, loss.numpy())\n",
    "            #    vis_imgs = visualize_result(\n",
    "            #        x=x[2], y=y[2], p=preds, img_title=img_test)\n",
    "            #    cv.imwrite(os.path.join(imgs_res_folder, 'results.png'), vis_imgs)\n",
    "            if step % 20 == 0 and loss < global_loss:  # 500\n",
    "                if epoch==0 and step==0:\n",
    "                    tmp_loss = np.array(t_loss)\n",
    "                    with train_writer.as_default():\n",
    "                        tf.summary.scalar('loss', tmp_loss.mean(), step=epoch)\n",
    "                        tf.summary.scalar('accuracy', accuracy.result(), step=epoch)\n",
    "\n",
    "                save_ckpt_path = os.path.join(checkpoint_dir, \"DexiNedL_model.h5\")\n",
    "                Model.save_weights(my_model, save_ckpt_path, save_format='h5')\n",
    "\n",
    "                global_loss = loss\n",
    "                print(\"Model saved in:  \", save_ckpt_path, \"Current loss:\", global_loss.numpy())\n",
    "\n",
    "            iter += 1  # global iteration\n",
    "\n",
    "        t_loss = np.array(t_loss)\n",
    "        # train summary\n",
    "        if epoch!=0:\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar('loss', t_loss.mean(), step=epoch)\n",
    "                tf.summary.scalar('accuracy', accuracy.result(), step=epoch)\n",
    "\n",
    "        Model.save_weights(my_model, os.path.join(epoch_ckpt_dir, \"DexiNed{}_model.h5\".format(str(epoch))),\n",
    "                            save_format=ckpt_save_mode)\n",
    "        print(\"Epoch:\", epoch, \"Model saved in Loss: \", t_loss.mean())\n",
    "\n",
    "        # validation\n",
    "        t_val_loss = []\n",
    "        for i, (x_val, y_val) in enumerate(val_data):\n",
    "\n",
    "            pred_val = my_model(x_val)\n",
    "            v_logits, V_loss = pre_process_binary_cross_entropy(\n",
    "                loss_bc, pred_val, y_val, use_tf_loss=False)\n",
    "            accuracy_val.update_state(y_true=y_val, y_pred=v_logits[-1])\n",
    "            t_val_loss.append(V_loss.numpy())\n",
    "            if i == 7:\n",
    "                break\n",
    "        val_acc = accuracy_val.result()\n",
    "        t_val_loss = np.array(t_val_loss)\n",
    "        print(\"Epoch(validation):\", epoch, \"Val loss: \", t_val_loss.mean(),\n",
    "                \"Accuracy: \", val_acc.numpy())\n",
    "        # validation summary\n",
    "        with val_writer.as_default():\n",
    "            tf.summary.scalar('loss', t_val_loss.mean(), step=epoch)\n",
    "            tf.summary.scalar('accuracy', val_acc.numpy(), step=epoch)\n",
    "\n",
    "        # Reset metrics every epoch\n",
    "        accuracy.reset_states()\n",
    "        accuracy_val.reset_states()\n",
    "\n",
    "    #my_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48c53b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weight_init = tf.initializers.glorot_uniform()\n",
    "\n",
    "l2 = regularizers.l2\n",
    "w_decay=1e-3\n",
    "\n",
    "glorot_normal = RandomNormal(stddev=0.01)\n",
    "\n",
    "orthogonal = tf.keras.initializers.Orthogonal(\n",
    "    gain=1.0, seed=None\n",
    ")\n",
    "\n",
    "self_conv2D_3 = Conv2D(32 , kernel_size=(3,3),strides=(2,2),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_batchnormalization_24 = BatchNormalization()\n",
    "self_conv2D_40 = Conv2D(64 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True,kernel_initializer= glorot_normal)\n",
    "self_batchnormalization_18 = BatchNormalization()\n",
    "self_activation_11 = Activation(activation='relu')\n",
    "self_conv2D_1 = Conv2D(128 , kernel_size=(3,3),strides=(1,1),padding = 'same')\n",
    "self_conv2D_16 = Conv2D(128 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_51 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_1 = BatchNormalization()\n",
    "self_transpoze2D_9 = Conv2DTranspose(1 , kernel_size=(2,2),strides=(2,2),padding = 'same')\n",
    "self_conv2D_49 = Conv2D(128 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_8 = BatchNormalization()\n",
    "self_activation_1 = Activation(activation='relu')\n",
    "self_conv2D_17 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_maxpool2D_1 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_transpoze2D_10 = Conv2DTranspose(1 , kernel_size=(2,2),strides=(2,2),padding = 'same')\n",
    "self_conv2D_28 = Conv2D(256 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_36 = Conv2D(256 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_activation_8 = Activation(activation='relu')\n",
    "self_conv2D_47 = Conv2D(256 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_48 = Conv2D(512 , kernel_size=(1,1),strides=(2,2),padding = 'same')\n",
    "self_conv2D_33 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_6 = BatchNormalization()\n",
    "self_activation_9 = Activation(activation='relu')\n",
    "self_conv2D_31 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_22 = BatchNormalization()\n",
    "self_activation_6 = Activation(activation='relu')\n",
    "self_conv2D_30 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_4 = BatchNormalization()\n",
    "self_activation_7 = Activation(activation='relu')\n",
    "self_conv2D_18 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_20 = BatchNormalization()\n",
    "self_conv2D_38 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_maxpool2D_2 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_transpoze2D_15 = Conv2DTranspose(16 , kernel_size=(4,4),strides=(2,2),padding = 'same')\n",
    "self_conv2D_11 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_34 = Conv2D(512 , kernel_size=(1,1),strides=(2,2),padding = 'same',use_bias=True)\n",
    "self_conv2D_50 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_2 = Conv2DTranspose(1 , kernel_size=(4,4),strides=(2,2),padding = 'same')\n",
    "self_activation_10 = Activation(activation='relu')\n",
    "self_conv2D_32 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_10 = BatchNormalization()\n",
    "self_activation_5 = Activation(activation='relu')\n",
    "self_conv2D_27 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_19 = BatchNormalization()\n",
    "self_activation_24 = Activation(activation='relu')\n",
    "self_conv2D_26 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_13 = BatchNormalization()\n",
    "self_activation_16 = Activation(activation='relu')\n",
    "self_conv2D_13 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_17 = BatchNormalization()\n",
    "self_activation_22 = Activation(activation='relu')\n",
    "self_conv2D_29 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_15 = BatchNormalization()\n",
    "self_activation_23 = Activation(activation='relu')\n",
    "self_conv2D_24 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_12 = BatchNormalization()\n",
    "self_conv2D_44 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_maxpool2D_3 = MaxPool2D(pool_size=(3,3),strides=(2,2),padding = 'same')\n",
    "self_transpoze2D_7 = Conv2DTranspose(16 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_conv2D_45 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_activation_15 = Activation(activation='relu')\n",
    "self_conv2D_6 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_2 = Conv2D(512 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_8 = Conv2DTranspose(16 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_conv2D_43 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_41 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_7 = BatchNormalization()\n",
    "self_transpoze2D_14 = Conv2DTranspose(1 , kernel_size=(8,8),strides=(2,2),padding = 'same')\n",
    "self_activation_17 = Activation(activation='relu')\n",
    "self_conv2D_19 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_16 = BatchNormalization()\n",
    "self_activation_13 = Activation(activation='relu')\n",
    "self_conv2D_21 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_14 = BatchNormalization()\n",
    "self_activation_14 = Activation(activation='relu')\n",
    "self_conv2D_15 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_5 = BatchNormalization()\n",
    "self_activation_18 = Activation(activation='relu')\n",
    "self_conv2D_14 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_26 = BatchNormalization()\n",
    "self_activation_19 = Activation(activation='relu')\n",
    "self_conv2D_5 = Conv2D(512 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_3 = BatchNormalization()\n",
    "self_conv2D_8 = Conv2D(256 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_35 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_activation_20 = Activation(activation='relu')\n",
    "self_transpoze2D_4 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_22 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_12 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_11 = BatchNormalization()\n",
    "self_transpoze2D_12 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_activation_21 = Activation(activation='relu')\n",
    "self_conv2D_37 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_conv2D_10 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_5 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_batchnormalization_21 = BatchNormalization()\n",
    "self_conv2D_42 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_13 = Conv2DTranspose(1 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_activation_3 = Activation(activation='relu')\n",
    "self_conv2D_20 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_9 = BatchNormalization()\n",
    "self_activation_4 = Activation(activation='relu')\n",
    "self_conv2D_23 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_2 = BatchNormalization()\n",
    "self_activation_2 = Activation(activation='relu')\n",
    "self_conv2D_7 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_23 = BatchNormalization()\n",
    "self_activation_12 = Activation(activation='relu')\n",
    "self_conv2D_25 = Conv2D(256 , kernel_size=(3,3),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_batchnormalization_25 = BatchNormalization()\n",
    "self_conv2D_39 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_11 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_4 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_3 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_9 = Conv2D(16 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_6 = Conv2DTranspose(16 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "self_conv2D_46 = Conv2D(1 , kernel_size=(1,1),strides=(1,1),padding = 'same',use_bias=True)\n",
    "self_transpoze2D_1 = Conv2DTranspose(1 , kernel_size=(16,16),strides=(2,2),padding = 'same')\n",
    "#self_block_cat = SingleConvBlock(\n",
    "#    1,k_size=(1,1),stride=(1,1),\n",
    "#    w_init=tf.constant_initializer(1/5))\n",
    "\n",
    "self_last_conv = Conv2D(1, kernel_size=(1,1), strides=(1,1), padding = 'same', kernel_initializer=tf.constant_initializer(1/5))\n",
    "self_last_batchnormalization = BatchNormalization()\n",
    "self_last_activation = Activation(activation='relu')\n",
    "\n",
    "#-------------------------------------\n",
    "\n",
    "x = Input(shape=(16,16,3))\n",
    "conv2D_3 = self_conv2D_3(x)\n",
    "batchnormalization_24 = self_batchnormalization_24(conv2D_3)\n",
    "conv2D_40 = self_conv2D_40(batchnormalization_24)\n",
    "batchnormalization_18 = self_batchnormalization_18(conv2D_40)\n",
    "activation_11 = self_activation_11(batchnormalization_18)\n",
    "conv2D_1 = self_conv2D_1(activation_11)\n",
    "conv2D_16 = self_conv2D_16(activation_11)\n",
    "conv2D_51 = self_conv2D_51(activation_11)\n",
    "batchnormalization_1 = self_batchnormalization_1(conv2D_1)\n",
    "transpoze2D_9 = self_transpoze2D_9(conv2D_51)\n",
    "conv2D_49 = self_conv2D_49(batchnormalization_1)\n",
    "batchnormalization_8 = self_batchnormalization_8(conv2D_49)\n",
    "activation_1 = self_activation_1(batchnormalization_8)\n",
    "conv2D_17 = self_conv2D_17(activation_1)\n",
    "maxpool2D_1 = self_maxpool2D_1(activation_1)\n",
    "transpoze2D_10 = self_transpoze2D_10(conv2D_17)\n",
    "add_1 = Add()([conv2D_16,maxpool2D_1])\n",
    "conv2D_28 = self_conv2D_28(maxpool2D_1)\n",
    "conv2D_36 = self_conv2D_36(maxpool2D_1)\n",
    "activation_8 = self_activation_8(add_1)\n",
    "conv2D_47 = self_conv2D_47(add_1)\n",
    "conv2D_48 = self_conv2D_48(conv2D_36)\n",
    "conv2D_33 = self_conv2D_33(activation_8)\n",
    "batchnormalization_6 = self_batchnormalization_6(conv2D_33)\n",
    "activation_9 = self_activation_9(batchnormalization_6)\n",
    "conv2D_31 = self_conv2D_31(activation_9)\n",
    "batchnormalization_22 = self_batchnormalization_22(conv2D_31)\n",
    "average_5 = Average()([batchnormalization_22,conv2D_28])\n",
    "activation_6 = self_activation_6(average_5)\n",
    "conv2D_30 = self_conv2D_30(activation_6)\n",
    "batchnormalization_4 = self_batchnormalization_4(conv2D_30)\n",
    "activation_7 = self_activation_7(batchnormalization_4)\n",
    "conv2D_18 = self_conv2D_18(activation_7)\n",
    "batchnormalization_20 = self_batchnormalization_20(conv2D_18)\n",
    "average_4 = Average()([batchnormalization_20,conv2D_28])\n",
    "conv2D_38 = self_conv2D_38(average_4)\n",
    "maxpool2D_2 = self_maxpool2D_2(average_4)\n",
    "transpoze2D_15 = self_transpoze2D_15(conv2D_38)\n",
    "add_2 = Add()([conv2D_47,maxpool2D_2])\n",
    "add_4 = Add()([conv2D_36,maxpool2D_2])\n",
    "conv2D_11 = self_conv2D_11(transpoze2D_15)\n",
    "conv2D_34 = self_conv2D_34(add_2)\n",
    "conv2D_50 = self_conv2D_50(add_4)\n",
    "transpoze2D_2 = self_transpoze2D_2(conv2D_11)\n",
    "activation_10 = self_activation_10(add_2)\n",
    "conv2D_32 = self_conv2D_32(activation_10)\n",
    "batchnormalization_10 = self_batchnormalization_10(conv2D_32)\n",
    "activation_5 = self_activation_5(batchnormalization_10)\n",
    "conv2D_27 = self_conv2D_27(activation_5)\n",
    "batchnormalization_19 = self_batchnormalization_19(conv2D_27)\n",
    "average_3 = Average()([batchnormalization_19,conv2D_50])\n",
    "activation_24 = self_activation_24(average_3)\n",
    "conv2D_26 = self_conv2D_26(activation_24)\n",
    "batchnormalization_13 = self_batchnormalization_13(conv2D_26)\n",
    "activation_16 = self_activation_16(batchnormalization_13)\n",
    "conv2D_13 = self_conv2D_13(activation_16)\n",
    "batchnormalization_17 = self_batchnormalization_17(conv2D_13)\n",
    "average_2 = Average()([batchnormalization_17,conv2D_50])\n",
    "activation_22 = self_activation_22(average_2)\n",
    "conv2D_29 = self_conv2D_29(activation_22)\n",
    "batchnormalization_15 = self_batchnormalization_15(conv2D_29)\n",
    "activation_23 = self_activation_23(batchnormalization_15)\n",
    "conv2D_24 = self_conv2D_24(activation_23)\n",
    "batchnormalization_12 = self_batchnormalization_12(conv2D_24)\n",
    "average_11 = Average()([batchnormalization_12,conv2D_50])\n",
    "conv2D_44 = self_conv2D_44(average_11)\n",
    "maxpool2D_3 = self_maxpool2D_3(average_11)\n",
    "transpoze2D_7 = self_transpoze2D_7(conv2D_44)\n",
    "add_3 = Add()([conv2D_34,maxpool2D_3])\n",
    "add_5 = Add()([conv2D_48,maxpool2D_3])\n",
    "conv2D_45 = self_conv2D_45(transpoze2D_7)\n",
    "activation_15 = self_activation_15(add_3)\n",
    "conv2D_6 = self_conv2D_6(add_3)\n",
    "conv2D_2 = self_conv2D_2(add_5)\n",
    "transpoze2D_8 = self_transpoze2D_8(conv2D_45)\n",
    "conv2D_43 = self_conv2D_43(activation_15)\n",
    "conv2D_41 = self_conv2D_41(transpoze2D_8)\n",
    "batchnormalization_7 = self_batchnormalization_7(conv2D_43)\n",
    "transpoze2D_14 = self_transpoze2D_14(conv2D_41)\n",
    "activation_17 = self_activation_17(batchnormalization_7)\n",
    "conv2D_19 = self_conv2D_19(activation_17)\n",
    "batchnormalization_16 = self_batchnormalization_16(conv2D_19)\n",
    "average_7 = Average()([batchnormalization_16,conv2D_2])\n",
    "activation_13 = self_activation_13(average_7)\n",
    "conv2D_21 = self_conv2D_21(activation_13)\n",
    "batchnormalization_14 = self_batchnormalization_14(conv2D_21)\n",
    "activation_14 = self_activation_14(batchnormalization_14)\n",
    "conv2D_15 = self_conv2D_15(activation_14)\n",
    "batchnormalization_5 = self_batchnormalization_5(conv2D_15)\n",
    "average_6 = Average()([batchnormalization_5,conv2D_2])\n",
    "activation_18 = self_activation_18(average_6)\n",
    "conv2D_14 = self_conv2D_14(activation_18)\n",
    "batchnormalization_26 = self_batchnormalization_26(conv2D_14)\n",
    "activation_19 = self_activation_19(batchnormalization_26)\n",
    "conv2D_5 = self_conv2D_5(activation_19)\n",
    "batchnormalization_3 = self_batchnormalization_3(conv2D_5)\n",
    "average_1 = Average()([batchnormalization_3,conv2D_2])\n",
    "add_6 = Add()([average_1,conv2D_6])\n",
    "conv2D_8 = self_conv2D_8(average_1)\n",
    "conv2D_35 = self_conv2D_35(average_1)\n",
    "activation_20 = self_activation_20(add_6)\n",
    "transpoze2D_4 = self_transpoze2D_4(conv2D_35)\n",
    "conv2D_22 = self_conv2D_22(activation_20)\n",
    "conv2D_12 = self_conv2D_12(transpoze2D_4)\n",
    "batchnormalization_11 = self_batchnormalization_11(conv2D_22)\n",
    "transpoze2D_12 = self_transpoze2D_12(conv2D_12)\n",
    "activation_21 = self_activation_21(batchnormalization_11)\n",
    "conv2D_37 = self_conv2D_37(transpoze2D_12)\n",
    "conv2D_10 = self_conv2D_10(activation_21)\n",
    "transpoze2D_5 = self_transpoze2D_5(conv2D_37)\n",
    "batchnormalization_21 = self_batchnormalization_21(conv2D_10)\n",
    "conv2D_42 = self_conv2D_42(transpoze2D_5)\n",
    "average_10 = Average()([batchnormalization_21,conv2D_8])\n",
    "transpoze2D_13 = self_transpoze2D_13(conv2D_42)\n",
    "activation_3 = self_activation_3(average_10)\n",
    "conv2D_20 = self_conv2D_20(activation_3)\n",
    "batchnormalization_9 = self_batchnormalization_9(conv2D_20)\n",
    "activation_4 = self_activation_4(batchnormalization_9)\n",
    "conv2D_23 = self_conv2D_23(activation_4)\n",
    "batchnormalization_2 = self_batchnormalization_2(conv2D_23)\n",
    "average_9 = Average()([batchnormalization_2,conv2D_8])\n",
    "activation_2 = self_activation_2(average_9)\n",
    "conv2D_7 = self_conv2D_7(activation_2)\n",
    "batchnormalization_23 = self_batchnormalization_23(conv2D_7)\n",
    "activation_12 = self_activation_12(batchnormalization_23)\n",
    "conv2D_25 = self_conv2D_25(activation_12)\n",
    "batchnormalization_25 = self_batchnormalization_25(conv2D_25)\n",
    "average_8 = Average()([batchnormalization_25,conv2D_8])\n",
    "conv2D_39 = self_conv2D_39(average_8)\n",
    "transpoze2D_11 = self_transpoze2D_11(conv2D_39)\n",
    "conv2D_4 = self_conv2D_4(transpoze2D_11)\n",
    "transpoze2D_3 = self_transpoze2D_3(conv2D_4)\n",
    "conv2D_9 = self_conv2D_9(transpoze2D_3)\n",
    "transpoze2D_6 = self_transpoze2D_6(conv2D_9)\n",
    "conv2D_46 = self_conv2D_46(transpoze2D_6)\n",
    "transpoze2D_1 = self_transpoze2D_1(conv2D_46)\n",
    "#tmp = [transpoze2D_1,transpoze2D_2,transpoze2D_9,transpoze2D_10,transpoze2D_13,transpoze2D_14]\n",
    "tmp = [transpoze2D_9,transpoze2D_10,transpoze2D_2,transpoze2D_14,transpoze2D_13,transpoze2D_1]\n",
    "\n",
    "#-----------\n",
    "#concatenate_1 = tf.concat(tmp,3)\n",
    "#-----------\n",
    "#concat_lambda = lambda xs: tf.concat(xs, axis=3)\n",
    "#concatenate_1 = tf.keras.layers.Lambda(concat_lambda)(tmp,3)\n",
    "#-----------\n",
    "self_concatenate_1 = Concatenate(axis=3)\n",
    "concatenate_1 = self_concatenate_1(tmp)\n",
    "\n",
    "#print(f\"concatenate_1 shape: {concatenate_1.shape}\")\n",
    "#return concatenate_1\n",
    "\n",
    "#results = [transpoze2D_1,transpoze2D_2,transpoze2D_9,transpoze2D_10,transpoze2D_13,transpoze2D_14]\n",
    "#block_cat = tf.concat(results, 3)  # BxHxWX6\n",
    "#print(f\"CONCATENATE_1 shape: {block_cat.shape}\")\n",
    "#block_cat = self_block_cat(block_cat)  # BxHxWX1\n",
    "#results.append(block_cat)\n",
    "\n",
    "\n",
    "last_conv =self_last_conv(concatenate_1)\n",
    "last_batchnormalization = self_last_batchnormalization(last_conv)\n",
    "last_activation = self_last_activation(last_batchnormalization)\n",
    "#concatenate_1 = Concatenate()([transpoze2D_1,transpoze2D_3,transpoze2D_4,transpoze2D_7,transpoze2D_8,transpoze2D_10])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs =x , outputs=last_activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef598a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_binary_cross_entropy(bc_loss,input, label, use_tf_loss=False):\n",
    "    # preprocess data\n",
    "    print(input.shape,label.shape)\n",
    "    y = label\n",
    "    loss = 0\n",
    "    w_loss=1.0\n",
    "    preds = []\n",
    "    \n",
    "    #print(label.shape)\n",
    "    #for f, b in zip(foo, bar):\n",
    "    #print(f, b)\n",
    "    for (tmp_p,tmpy) in zip(input,y):\n",
    "        print(\">\",tmp_p.shape,tmpy.shape)\n",
    "        # tmp_p = input[i]\n",
    "\n",
    "        # loss processing\n",
    "        tmp_y = tf.cast(tmpy, dtype=tf.float32)\n",
    "        print(tmp_y.numpy())\n",
    "        mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "        h,w,c=mask.get_shape()\n",
    "        #plt.imshow(mask)\n",
    "        positives = tf.math.reduce_sum(mask, axis=[1,2], keepdims=True)\n",
    "        negatives = h*w-positives\n",
    "\n",
    "        beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "        beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "        pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "        logits = tf.sigmoid(tmp_p)\n",
    "        #print(tmp_y.shape,logits.shape)\n",
    "        l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "                         sample_weight=pos_w)\n",
    "\n",
    "        preds.append(logits)\n",
    "        loss += (l_cost*w_loss)\n",
    "\n",
    "    return preds, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0b65d-6113-4508-84ea-f63162cecb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_binary_cross_entropy(bc_loss,input, label, use_tf_loss=False):\n",
    "    # preprocess data\n",
    "    y = label\n",
    "    loss = 0\n",
    "    w_loss=1.0\n",
    "    preds = []\n",
    "    #for tmp_p in input:\n",
    "    # tmp_p = input[i]\n",
    "    tmp_p = input\n",
    "    # loss processing\n",
    "    tmp_y = tf.cast(y, dtype=tf.float32)\n",
    "    mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "    b,h,w,c=mask.get_shape()\n",
    "    positives = tf.math.reduce_sum(mask, axis=[1, 2, 3], keepdims=True)\n",
    "    negatives = h*w*c-positives\n",
    "\n",
    "    beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "    beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "    pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "    logits = tf.sigmoid(tmp_p)\n",
    "\n",
    "    l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "                     sample_weight=pos_w)\n",
    "\n",
    "    preds.append(logits)\n",
    "    loss += (l_cost*w_loss)\n",
    "\n",
    "    return preds, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee83c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = DataLoader(data_path, \"train_list.txt\", \"blender1\", 12, 12, 4, True)\n",
    "# val_data = DataLoader(data_path, \"val_list.txt\", \"blender2\", 12, 12, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1dc59c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 16, 16, 1) (2, 16, 16, 1)\n",
      "> (16, 16, 1) (16, 16, 1)\n",
      "[[[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.03921569]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.16078432]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.02745098]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.03137255]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.56078434]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.03921569]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]]\n",
      "> (16, 16, 1) (16, 16, 1)\n",
      "[[[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.10588235]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.54901963]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.3372549 ]\n",
      "  [0.5647059 ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.14117648]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.42352942]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.41960785]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.23137255]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.25882354]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 02:02:16.910656: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Equal_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Equal] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      2\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, run_eagerly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdexined_try\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, model_name, train_data, test_data, lr, beta1, max_epochs, batch_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m     pred \u001b[38;5;241m=\u001b[39m my_model(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m     preds, loss \u001b[38;5;241m=\u001b[39m pre_process_binary_cross_entropy(\n\u001b[1;32m     52\u001b[0m         loss_bc, pred, y, use_tf_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43maccuracy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, my_model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, my_model\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py:77\u001b[0m, in \u001b[0;36mupdate_state_wrapper.<locals>.decorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to run metric.update_state in replica context when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe metric was not created in TPUStrategy scope. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure the keras Metric is created in TPUstrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscope. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39mgraph_context_for_symbolic_tensors(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 77\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_state_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m     79\u001b[0m     result \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39mget_operations()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py:140\u001b[0m, in \u001b[0;36mMetric.__new__.<locals>.update_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m control_status \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    137\u001b[0m ag_update_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    138\u001b[0m     obj_update_state, control_status\n\u001b[1;32m    139\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_update_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py:723\u001b[0m, in \u001b[0;36mMeanMetricWrapper.update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    716\u001b[0m y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[1;32m    717\u001b[0m     y_pred, y_true\n\u001b[1;32m    718\u001b[0m )\n\u001b[1;32m    720\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    722\u001b[0m )\n\u001b[0;32m--> 723\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mag_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(matches)\n\u001b[1;32m    725\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mapply_valid_mask(\n\u001b[1;32m    726\u001b[0m     matches, sample_weight, mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction\n\u001b[1;32m    727\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py:932\u001b[0m, in \u001b[0;36mbinary_matches\u001b[0;34m(y_true, y_pred, threshold)\u001b[0m\n\u001b[1;32m    930\u001b[0m threshold \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(threshold, y_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    931\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(y_pred \u001b[38;5;241m>\u001b[39m threshold, y_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequal\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m, backend\u001b[38;5;241m.\u001b[39mfloatx())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Equal_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:Equal] name: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", run_eagerly=True)\n",
    "\n",
    "train(model,\"dexined_try\",train_data,val_data,0.0001,0.5,2,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a2bf2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> (16, 16, 3) (16, 16, 1)\n",
      "[[[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.10588235]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.54901963]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.3372549 ]\n",
      "  [0.5647059 ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.14117648]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.42352942]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.41960785]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.23137255]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.25882354]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]]\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]]\n",
      "[[[10.]]\n",
      "\n",
      " [[ 8.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[ 7.]]\n",
      "\n",
      " [[14.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[13.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[14.]]]\n",
      "> (16, 16, 3) (16, 16, 1)\n",
      "[[[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.11764706]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.01176471]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.1254902 ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.05882353]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.20784314]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.09803922]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.57254905]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.5764706 ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.03137255]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.24313726]\n",
      "  [0.23921569]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]]\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]]\n",
      "[[[10.]]\n",
      "\n",
      " [[ 7.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[ 7.]]\n",
      "\n",
      " [[13.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[13.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[13.]]]\n",
      "> (16, 16, 3) (16, 16, 1)\n",
      "[[[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.03921569]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.16078432]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.02745098]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.03137255]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.56078434]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.03921569]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]]\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]]\n",
      "[[[10.]]\n",
      "\n",
      " [[ 7.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[ 7.]]\n",
      "\n",
      " [[13.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[13.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[12.]]]\n",
      "> (16, 16, 3) (16, 16, 1)\n",
      "[[[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.23137255]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.01176471]\n",
      "  [0.15294118]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.40392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.27058825]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.2784314 ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.05882353]\n",
      "  [0.00392157]]\n",
      "\n",
      " [[0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]\n",
      "  [0.00392157]]]\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]]\n",
      "[[[11.]]\n",
      "\n",
      " [[ 9.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[ 7.]]\n",
      "\n",
      " [[13.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[12.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[10.]]\n",
      "\n",
      " [[13.]]\n",
      "\n",
      " [[11.]]\n",
      "\n",
      " [[12.]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for step, (x, y) in enumerate(train_data):\n",
    "    plt.figure()\n",
    "    for (tmp_p,tmpy) in zip(x,y):\n",
    "        print(\">\",tmp_p.shape,tmpy.shape)\n",
    "        # tmp_p = input[i]\n",
    "\n",
    "        # loss processing\n",
    "        tmp_y = tf.cast(tmpy, dtype=tf.float32)\n",
    "        print(tmp_y.numpy())\n",
    "        plt.imshow(tmp_y)\n",
    "        mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "        h,w,c=mask.get_shape()\n",
    "        print(mask.numpy())\n",
    "        plt.imshow(mask)\n",
    "        positives = tf.math.reduce_sum(mask, axis=[1,2], keepdims=True)\n",
    "        negatives = h*w-positives\n",
    "        print(positives.numpy())\n",
    "        plt.imshow(positives)\n",
    "\n",
    "        #beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "        #beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "        #pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "        #logits = tf.sigmoid(tmp_p)\n",
    "        #print(tmp_y.shape,logits.shape)\n",
    "        #l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "        #                 sample_weight=pos_w)\n",
    "\n",
    "        #preds.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3be4e557-7bd6-4a6a-9d01-2e3d5543493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> (16, 16, 3) (2, 16, 16, 1)\n",
      "positives (2, 1, 1, 1)\n",
      "tf.Tensor(\n",
      "[[[[177.]]]\n",
      "\n",
      "\n",
      " [[[174.]]]], shape=(2, 1, 1, 1), dtype=float32)\n",
      "negatives (2, 1, 1, 1)\n",
      "tf.Tensor(\n",
      "[[[[79.]]]\n",
      "\n",
      "\n",
      " [[[82.]]]], shape=(2, 1, 1, 1), dtype=float32)\n",
      "> (16, 16, 3) (2, 16, 16, 1)\n",
      "positives (2, 1, 1, 1)\n",
      "tf.Tensor(\n",
      "[[[[177.]]]\n",
      "\n",
      "\n",
      " [[[174.]]]], shape=(2, 1, 1, 1), dtype=float32)\n",
      "negatives (2, 1, 1, 1)\n",
      "tf.Tensor(\n",
      "[[[[79.]]]\n",
      "\n",
      "\n",
      " [[[82.]]]], shape=(2, 1, 1, 1), dtype=float32)\n",
      "> (16, 16, 3) (2, 16, 16, 1)\n",
      "positives (2, 1, 1, 1)\n",
      "tf.Tensor(\n",
      "[[[[172.]]]\n",
      "\n",
      "\n",
      " [[[176.]]]], shape=(2, 1, 1, 1), dtype=float32)\n",
      "negatives (2, 1, 1, 1)\n",
      "tf.Tensor(\n",
      "[[[[84.]]]\n",
      "\n",
      "\n",
      " [[[80.]]]], shape=(2, 1, 1, 1), dtype=float32)\n",
      "> (16, 16, 3) (2, 16, 16, 1)\n",
      "positives (2, 1, 1, 1)\n",
      "tf.Tensor(\n",
      "[[[[172.]]]\n",
      "\n",
      "\n",
      " [[[176.]]]], shape=(2, 1, 1, 1), dtype=float32)\n",
      "negatives (2, 1, 1, 1)\n",
      "tf.Tensor(\n",
      "[[[[84.]]]\n",
      "\n",
      "\n",
      " [[[80.]]]], shape=(2, 1, 1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for step, (x, y) in enumerate(train_data):\n",
    "    for tmp_p in x:\n",
    "        print(\">\",tmp_p.shape,y.shape)\n",
    "        # tmp_p = input[i]\n",
    "\n",
    "        # loss processing\n",
    "        tmp_y = tf.cast(y, dtype=tf.float32)\n",
    "        mask = tf.dtypes.cast(tmp_y > 0., tf.float32)\n",
    "        b,h,w,c=mask.get_shape()\n",
    "        #plt.imshow(mask)\n",
    "        positives = tf.math.reduce_sum(mask, axis=[1,2,3], keepdims=True)\n",
    "        negatives = h*w*c-positives\n",
    "        print(\"positives\",positives.shape)\n",
    "        print(positives)\n",
    "        print(\"negatives\",negatives.shape)\n",
    "        print(negatives)\n",
    "        beta2 = (1.*positives) / (negatives + positives) # negatives in hed\n",
    "        beta = (1.1*negatives)/ (positives + negatives) # positives in hed\n",
    "        pos_w = tf.where(tf.equal(y, 0.0), beta2, beta)\n",
    "        logits = tf.sigmoid(tmp_p)\n",
    "        #print(tmp_y.shape,logits.shape)\n",
    "        #l_cost = bc_loss(y_true=tmp_y, y_pred=logits,\n",
    "        #                 sample_weight=pos_w)\n",
    "\n",
    "        #preds.append(logits)\n",
    "        #loss += (l_cost*w_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
